#!/usr/bin/env python3

"""
Traverses analysis output and loads traces and odor information into database.
"""

import os
from os.path import join, split
import glob
from datetime import datetime
import pickle
import warnings
import atexit
import time
import xml.etree.ElementTree as etree
import pprint

from sqlalchemy import create_engine, MetaData, Table
from sqlalchemy.sql import sqltypes
from sqlalchemy.dialects import postgresql
import h5py
import numpy as np
import pandas as pd
import matlab.engine
import git


use_cached_gsheet = True
show_inferred_paths = True
convert_h5 = True
calc_timing_info = False
motion_correct = True
only_motion_correct_for_analysis = True

store_multiple_analysis = False


analysis_started_at = time.time()

# TODO need to add stuff to path? what all?
# TODO future have a bug generally, or only if stopped w/ ctrl-d from ipdb like
# i had?
#future = matlab.engine.start_matlab(async=True)
#evil = None
evil = matlab.engine.start_matlab()
atexit.register(evil.quit)

# The latter is here so I can tell which files within it are actually needed
# in the current version of the analysis.
exclude_from_matlab_path = {'CaImAn-MATLAB','matlab_helper_functions'}
#
userpath = evil.userpath()
for root, dirs, _ in os.walk(userpath, topdown=True):
    dirs[:] = [d for d in dirs if (not d.startswith('.') and
        not d.startswith('@') and not d.startswith('+') and
        d not in exclude_from_matlab_path)]

    evil.addpath(root)

# To get Git version information to have a record of what analysis was
# performed.
matlab_repo_name = 'matlab_kc_plane'
matlab_code_path = join(userpath, matlab_repo_name)
#

url = 'postgresql+psycopg2://tracedb:tracedb@localhost:5432/tracedb'
# TODO should this be closed too in atexit?
conn = create_engine(url)


def to_sql_with_duplicates(new_df, table_name, index=False):
    # Other columns should be generated by database anyway.
    cols = list(new_df.columns)
    if index:
        cols += list(new_df.index.names)
    table_cols = ', '.join(cols)

    md = MetaData()
    table = Table(table_name, md, autoload_with=conn)
    dtypes = {c.name: c.type for c in table.c}
    '''
    print('SQL column types:')
    pprint.pprint(dtypes)
    '''
   
    df_types = new_df.dtypes.to_dict()
    if index:
        df_types.update({n: new_df.index.get_level_values(n).dtype
            for n in new_df.index.names})

    '''
    print('\nOld dataframe column types:')
    pprint.pprint(df_types)
    '''

    # TODO TODO convert each df_type to sqlalchemy type
    # TODO problems w/ using types as keys? (is vs == check?)
    '''
    sqltypes.INTEGER: np.dtype('int32'),
    sqltypes.SMALLINT: np.dtype('int16'),
    sqltypes.REAL: np.dtype('float32'),
    postgresql.base.DOUBLE_PRECISION: np.dtype('float64'),
    postgresql.base.TIMESTAMP: np.dtype('<M8[ns]')
    '''
    sqlalchemy2pd_type = {
        'INTEGER()': np.dtype('int32'),
        'SMALLINT()': np.dtype('int16'),
        'REAL()': np.dtype('float32'),
        'DOUBLE_PRECISION(precision=53)': np.dtype('float64'),
        # TODO but maybe this was the type causing the problem?
        'DATE()': np.dtype('<M8[ns]')
    }
    #pprint.pprint(sqlalchemy2pd_type)

    new_df_types = {n: sqlalchemy2pd_type[repr(t)] for n, t in dtypes.items()
        if repr(t) in sqlalchemy2pd_type}

    '''
    print('\nNew dataframe column types:')
    pprint.pprint(new_df_types)
    '''

    # TODO how to get around converting things to int if they have NaN.
    # possible to not convert?
    new_column_types = dict()
    new_index_types = dict()
    for k, t in new_df_types.items():
        if k in new_df.columns and not new_df[k].isnull().any():
            new_column_types[k] = t

        # TODO or is it always true that index level can't be NaN anyway?
        elif (k in new_df.index.names and
            not new_df.index.get_level_values(k).isnull().any()):

            new_index_types[k] = t

        # TODO print types being skipped b/c nan?

    '''
    print('')
    print(sorted(new_df.columns))
    print(sorted(new_df.index.names))
    print(sorted(new_column_types.keys()))
    print(sorted(new_index_types.keys()))
    '''
    new_df = new_df.astype(new_column_types, copy=False)
    if index:
        # TODO need to handle case where conversion dict is empty
        # (seems to fail?)
        #pprint.pprint(new_index_types)

        #new_df.index = new_df.index.astype(new_index_types, copy=False)

        # MultiIndex astype method seems to not work the same way?
        new_df.index = pd.MultiIndex.from_frame(
            new_df.index.to_frame().astype(new_index_types, copy=False))

    # TODO print the type of any sql types not convertible?
    # TODO assert all dtypes can be converted w/ this dict

    if index:
        print('writing to temporary table temp_{}...'.format(table_name))

    # TODO figure out how to profile
    new_df.to_sql('temp_' + table_name, conn, if_exists='replace', index=index,
        dtype=dtypes)

    # TODO change to just get column names?
    query = '''
    SELECT a.attname, format_type(a.atttypid, a.atttypmod) AS data_type
    FROM   pg_index i
    JOIN   pg_attribute a ON a.attrelid = i.indrelid
        AND a.attnum = ANY(i.indkey)
    WHERE  i.indrelid = '{}'::regclass
    AND    i.indisprimary;
    '''.format(table_name)
    result = conn.execute(query)
    pk_cols = ', '.join([n for n, _ in result])

    # TODO prefix w/ ANALYZE EXAMINE and look at results
    query = ('INSERT INTO {0} ({1}) SELECT {1} FROM temp_{0} ' +
        'ON CONFLICT ({2}) DO NOTHING').format(table_name, table_cols, pk_cols)
    if index:
        print('inserting into {} from temporary table... '.format(table_name),
            end='')

    # TODO let this happen async in the background? (don't need result)
    conn.execute(query)

    if index:
        print('done')

    # TODO drop staging table


#matlab_output_file = 'test_data/struct_no_sparsearray.mat'
####matlab_output_file = 'test_data/_007_cnmf.mat'

gsheet_cache_file = '.gsheet_cache.p'
if use_cached_gsheet and os.path.exists(gsheet_cache_file):
    print('Loading Google sheet data from cache at {}'.format(
        gsheet_cache_file))

    with open(gsheet_cache_file, 'rb') as f:
        sheets = pickle.load(f)

else:
    with open('google_sheet_link.txt', 'r') as f:
        gsheet_link = f.readline().split('/edit')[0] + '/export?format=csv&gid='

    # If you want to add more sheets, when you select the new sheet in your
    # browser, the GID will be at the end of the URL in the address bar.
    sheet_gids = {
        'fly_preps': '269082112',
        'recordings': '0',
        'daily_settings': '229338960'
    }

    # TODO flag to cache these to just be nice to google?
    sheets = dict()
    for df_name, gid in sheet_gids.items():
        df = pd.read_csv(gsheet_link + gid)

        # TODO convert any other dtypes?
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])

        # TODO complain if there are any missing fly_nums

        sheets[df_name] = df

    boolean_columns = {
        'used_for_analysis',
        'raw_data_discarded',
        'raw_data_lost'
    }
    na_cols = list(set(sheets['recordings'].columns) - boolean_columns)
    sheets['recordings'].dropna(how='all', subset=na_cols, inplace=True)

    with open(gsheet_cache_file, 'wb') as f:
        pickle.dump(sheets, f)

# TODO maybe make df some merge of the three sheets?
df = sheets['recordings']

df.raw_data_discarded = df.raw_data_discarded.fillna(False)
# TODO say when this happens?
df.drop(df[df.raw_data_discarded].index, inplace=True)

# TODO TODO warn / fail if 'used_for_analysis' and either discard / lost is
# checked

# Not sure where there were any NaN here anyway...
df.raw_data_lost = df.raw_data_lost.fillna(False)
df.drop(df[df.raw_data_lost].index, inplace=True)

keys = ['date', 'fly_num']
df['recording_num'] = df.groupby(keys).cumcount() + 1
# Since otherwise cumcount seems to be zero for stuff without a group...
# (i.e. w/ one of the keys null)
df.loc[pd.isnull(df[keys]).any(axis=1), 'recording_num'] = np.nan

# TODO delete hack after dealing w/ remy's conventions (some of which were
# breaking the code assuming my conventions)
df.drop(df[df.project != 'natural_odors'].index, inplace=True)

if show_inferred_paths:
    missing_thorimage = pd.isnull(df.thorimage_dir)
    missing_thorsync = pd.isnull(df.thorsync_dir)


df['thorimage_num'] = df.thorimage_dir.apply(lambda x: np.nan if pd.isnull(x)
    else int(x[1:]))
df['numbering_consistent'] = \
    pd.isnull(df.thorimage_num) | (df.thorimage_num == df.recording_num)

# TODO unit test this
# TODO TODO check that, if there are mismatches here, that they *never* happen
# when recording num will be used for inference in rows in the group *after*
# the mismatch
gkeys = keys + ['thorimage_dir','thorsync_dir','thorimage_num',
                'recording_num','numbering_consistent']
for name, group_df in df.groupby(keys):
    '''
    # case 1: all consistent
    # case 2: not all consistent, but all thorimage_dir filled in
    # case 3: not all consistent, but just because thorimage_dir was null
    '''
    #print(group_df[gkeys])

    # TODO check that first_mismatch based approach includes this case
    #if pd.notnull(group_df.thorimage_dir).all():
    #    continue

    mismatches = np.argwhere(~ group_df.numbering_consistent)
    if len(mismatches) == 0:
        continue

    first_mismatch_idx = mismatches[0][0]
    #print('first_mismatch:\n', group_df[gkeys].iloc[first_mismatch_idx])

    # TODO test case where the first mismatch is last
    following_thorimage_dirs = group_df.thorimage_dir.iloc[first_mismatch_idx:]
    #print('checking these are not null:\n', following_thorimage_dirs)
    assert pd.notnull(following_thorimage_dirs).all()

df.thorsync_dir.fillna(df.thorimage_num.apply(lambda x: np.nan if pd.isnull(x)
    else 'SyncData{:03d}'.format(int(x))), inplace=True)

df.drop(columns=['thorimage_num','numbering_consistent'], inplace=True)


# TODO TODO check for conditions in which we might need to renumber recording
# num? (dupes / any entered numbers along the way that are inconsistent w/
# recording_num results)
df.thorimage_dir.fillna(df.recording_num.apply(lambda x: np.nan if pd.isnull(x)
    else'_{:03d}'.format(int(x))), inplace=True)

df.thorsync_dir.fillna(df.recording_num.apply(lambda x: np.nan if pd.isnull(x)
    else 'SyncData{:03d}'.format(int(x))), inplace=True)

if show_inferred_paths:
    cols = ['date','fly_num','thorimage_dir','thorsync_dir']
    print('Inferred ThorImage directories:')
    print(df.loc[missing_thorimage, cols])
    print('\nInferred ThorSync directories:')
    print(df.loc[missing_thorsync, cols])
    print('')

keys = ['date','fly_num']
duped_thorimage = df.duplicated(subset=keys + ['thorimage_dir'], keep=False)
duped_thorsync = df.duplicated(subset=keys + ['thorsync_dir'], keep=False)

try:
    assert not duped_thorimage.any()
    assert not duped_thorsync.any()
except AssertionError:
    print('Duplicated ThorImage directories after path inference:')
    print(df[duped_thorimage])
    print('\nDuplicated ThorSync directories after path inference:')
    print(df[duped_thorsync])
    raise

sheets['fly_preps'].dropna(subset=['date','fly_num'], inplace=True)
to_sql_with_duplicates(sheets['fly_preps'].rename(
    columns={'date': 'prep_date'}), 'flies')


# TODO TODO warn if any raw data is not present on NAS / report which
# (could indicate problems w/ path inference)

# TODO add bool col to recordings to indicate whether stimulus order in python
# file was adhered to?
# TODO or just only use it for natural_odors project for now?
# (probably just want to ignore order for project="n/a (prep checking)" columns
# anyway)

# TODO + summarize those that still need analysis run on them (and run it?)
# TODO print stuff w/ used_for_analysis checked w/o either data in database or
# analysis output on disk (or just latter)

# TODO move these paths to config file...
raw_data_root = '/mnt/nas/mb_team/raw_data'
analysis_output_root = '/mnt/nas/mb_team/analysis_output'

rel_to_cnmf_mat = 'cnmf'

stimfile_root = '/mnt/nas/mb_team/stimulus_data_files' 

natural_odors_concentrations = pd.read_csv('natural_odor_panel_vial_concs.csv')
natural_odors_concentrations.set_index('name', inplace=True)

# TODO TODO loop over more stuff than just natural_odors / used_for_analysis
# to load all PID stuff in (will need pin info for prep checking, etc, exps)

# TODO complain if there are flies w/ used_for_analysis not checked w/o
# rejection reason

# TODO maybe don't err in this case (w/ option to only run on analysis?)
# and symmetric option for analysis root?
if not os.path.isdir(raw_data_root):
    raise IOError('raw_data_root {} does not exist'.format(
        raw_data_root))

if not os.path.isdir(analysis_output_root):
    raise IOError('analysis_output_root {} does not exist'.format(
        analysis_output_root))

if not os.path.isdir(stimfile_root):
    raise IOError('stimfile_root {} does not exist'.format(
        stimfile_root))

# TODO TODO also do a first pass over everything w/ default params so just
# manual correction remains

# TODO make fns that take date + fly_num + cwd then re-use iteration
# over date / fly_num (if mirrored data layout for raw / analysis)?

for full_fly_dir in glob.glob(raw_data_root + '/*/*/'):
    full_fly_dir = os.path.normpath(full_fly_dir)
    #print(full_fly_dir)
    prefix, fly_dir = split(full_fly_dir)
    _, date_dir = split(prefix)

    if fly_dir == 'unsorted':
        # TODO maybe make attempt to sort?
        continue

    try:
        fly_num = int(fly_dir)
    except ValueError:
        # TODO maybe warn if not in whitelist, like 'unsorted')
        continue

    try:
        date = datetime.strptime(date_dir, '%Y-%m-%d')
    except ValueError:
        continue

    print(date)
    print(fly_num)

    used = df.loc[df.used_for_analysis &
        (df.date == date) & (df.fly_num == fly_num)]

    # TODO better format for no used dirs case
    print('Used ThorImage dirs:')
    print(used.thorimage_dir)

    # TODO maybe do this in analysis actually? (to not just make a bunch of
    # empty dirs...)
    analysis_fly_dir = join(analysis_output_root, date_dir, fly_dir)
    if not os.path.isdir(analysis_fly_dir):
        # Will also make any necessary parent (date) directories.
        os.makedirs(analysis_fly_dir)

    # TODO maybe use regexp to check syncdata / util fn to check for name +
    # stuff in it?
    if convert_h5:
        ####print('Converting ThorSync HDF5 files to .mat...')
        for syncdir in glob.glob(join(full_fly_dir, 'SyncData*')):
            #print(syncdir)
            '''
            if evil is None:
                evil = future.result()
                userpath = evil.userpath()
                paths_before = set(evil.path().split(':'))
                for root, dirs, _ in os.walk(userpath, topdown=True):
                    dirs[:] = [d for d in dirs if (not d.startswith('.') and
                        not d.startswith('@') and not d.startswith('+') and
                        d not in exclude_from_matlab_path)]

                    evil.addpath(root)

                paths_after = set(evil.path().split(':'))
                pprint.pprint(paths_after - paths_before)
            '''

            # TODO make it so one ctrl-c closes whole program, rather than just
            # cancelling matlab function and continuing

            # Will immediately return if output already exists.
            evil.thorsync_h5_to_mat(syncdir, full_fly_dir, nargout=0)
            # TODO (check whether she is loosing information... file size really
            # shouldn't be that different. both hdf5...)

            # TODO do i want flags to disable *each* step separately for
            # unanalyzed stuff? just one flag? always ignore that stuff?

            # TODO remy could also convert xml here if she wanted
        ####print('Done converting ThorSync HDF5 to .mat\n')

    if calc_timing_info:
        for _, row in used[['thorimage_dir','thorsync_dir']].iterrows():
            thorimage_dir = join(full_fly_dir, row['thorimage_dir'])
            if not os.path.isdir(thorimage_dir):
                warnings.warn('thorimage_dir {} did not exist for recording ' +
                    'marked as used_for_analysis.')
                continue

            # If not always running h5->mat conversion first, will need to check
            # for the mat, rather than just thorsync_dir.
            thorsync_dir = join(full_fly_dir, row['thorsync_dir'])
            if not os.path.isdir(thorsync_dir):
                warnings.warn('thorsync_dir {} did not exist for recording ' +
                    'marked as used_for_analysis.')
                continue

            print('\nThorImage and ThorSync dirs for get_stiminfo:')
            print(thorimage_dir)
            print(thorsync_dir)
            print('')

            print(('getting stimulus timing information for {}, {}, {}...'
                ).format( date_dir, fly_num, row['thorimage_dir']), end='')

            '''
            # TODO delete
            print('\n')
            print(date_dir)
            print(date_dir == '2019-01-18')
            print(fly_num)
            print(fly_num == 2)
            print(row['thorsync_dir'])
            print(row['thorsync_dir'] == 'SyncData005')
            if not (date_dir == '2019-01-18' and fly_num == 2 and
                #print('SKIPPING KNOWN FAILING GET_STIMINFO CALL')
                print('SKIPPING LIKELY NON-FAILING GET_STIMINFO CALL')
                    row['thorsync_dir'] == 'SyncData005'):
                continue
            #
            '''

            # throwing everything into _<>_cnmf.mat, as we are, would need to
            # inspect it to check whether we already have the stiminfo...
            evil.get_stiminfo(thorimage_dir, row['thorsync_dir'],
                analysis_fly_dir, date_dir, fly_num, nargout=0)

            print(' done.')

    # TODO loop over thorimage dirs and make tifs from each if they don't exist
    # TODO TODO what all metadata is important in the tif? need to script imagej
    # to get most reliable tifs? some python / matlab fn work?
    '''
    for thorimage_dir in glob.glob(join(full_fly_dir, '_*/')):
        thorimage_id = split(os.path.normpath(thorimage_dir))[-1]
    '''

    # maybe avoid searching for thorimage dirs at all if there are no used 
    # rows for this (date,fly) combo, and only_motion_correct_for_analysis

    # TODO use multiple matlab instances to run normcore on different
    # directories in parallel?
    # TODO exclude stuff that indicates it's either already avg or motion
    # corrected? (or just always keep them separately?)
    if motion_correct:
        # TODO maybe also look w/o underscore, if that's remy's convention
        for input_tif_path in glob.glob(
            join(full_fly_dir, 'tif_stacks', '_*.tif')):

            if only_motion_correct_for_analysis:
                thorimage_id = split(input_tif_path)[-1][:-4]

                recordings = used[used.thorimage_dir == thorimage_id]
                if len(recordings) == 0:
                    continue

            print('\nRunning normcorre_tiff on', input_tif_path)
            # TODO only register one way by default? nonrigid? args to
            # configure?
            evil.normcorre_tiff(input_tif_path, analysis_fly_dir, nargout=0)
            #import ipdb; ipdb.set_trace()

    # TODO and if remy wants, copy thorimage xmls

    print('')

    # TODO maybe delete empty folders under analysis? (do in atexit handler)

#import sys; sys.exit()

def git_hash(repo_file):
    repo = git.Repo(repo_file, search_parent_directories=True)
    current_hash = repo.head.object.hexsha
    return current_hash
# TODO maybe also return current remote url, if listed? whether pushed?
# TODO store unsaved changes too? sep column? see my metatools package
'''
diff = repo.index.diff(None, create_patch=True)
exactly = 'exactly ' if len(diff) == 0 else ''
'''

this_repo_file = os.path.realpath(__file__)
this_repo_path = split(this_repo_file)[0]
current_hash = git_hash(this_repo_file)
matlab_hash = git_hash(matlab_code_path)

# TODO just store all data separately?
# TODO TODO maybe just use matlab code repo + description in analysis
# description that gets checked? (because that's what actually generates cnmf,
# which is used for responses)
analysis_description = '{}@{}\n{}@{}'.format(this_repo_path, current_hash,
    matlab_code_path, matlab_hash)

analyzed_at = datetime.fromtimestamp(analysis_started_at)

analysis_runs = pd.DataFrame({
    'analysis_description': [analysis_description],
    'analyzed_at': [analyzed_at]
})
# TODO don't insert into this if dependent stuff won't be written? same for some
# of the other metadata tables?
to_sql_with_duplicates(analysis_runs, 'analysis_runs')

# Need to do this as long as the part of the key indicating the
# analysis, in the recordings table, is generated by the database.
db_analysis_runs = pd.read_sql('analysis_runs', conn).set_index(
    'analysis_description')
analysis_run = \
    db_analysis_runs.loc[analysis_description, 'analysis_run']


# TODO diff between ** and */ ?
# TODO os.path.join + os invariant way of looping over dirs
for analysis_dir in glob.glob(analysis_output_root+ '/*/*/'):
    analysis_dir = os.path.normpath(analysis_dir)

    prefix, fly_dir = split(analysis_dir)
    _, date_dir = split(prefix)

    try:
        fly_num = int(fly_dir)
    except ValueError:
        continue

    try:
        date = datetime.strptime(date_dir, '%Y-%m-%d')
    except ValueError:
        continue

    print(analysis_dir)

    # TODO TODO complain if stuff marked as used for analysis is not found here
    for mat in glob.glob(join(analysis_dir, rel_to_cnmf_mat, '*_cnmf.mat')):
        print(mat)

        prefix = split(mat)[-1].split('_')[:-1]

        thorimage_id = '_' + prefix[1]

        recordings = df.loc[(df.date == date) & (df.fly_num == fly_num) &
                            (df.thorimage_dir == thorimage_id)]
        recording = recordings.iloc[0]

        if recording.project != 'natural_odors':
            warnings.warn('project type {} not supported. skipping.')
            continue


        raw_fly_dir = join(raw_data_root, date_dir, fly_dir)
        thorsync_dir = join(raw_fly_dir, recording['thorsync_dir'])
        thorimage_dir = join(raw_fly_dir, recording['thorimage_dir'])
        stimulus_data_path = join(stimfile_root,
                                  recording['stimulus_data_file'])

        # TODO for recordings.started_at, load time from one of the thorlabs
        # files
        # TODO check that ThorImageExperiment/Date/uTime parses to date field,
        # w/ appropriate timezone settings (or insert raw and check database has
        # something like date)?
        thorimage_xml_path = join(thorimage_dir, 'Experiment.xml')
        xml_root = etree.parse(thorimage_xml_path).getroot()
        started_at = \
            datetime.fromtimestamp(float(xml_root.find('Date').attrib['uTime']))


        # TODO TODO test this actually avoids insertion in case where analysis
        # is up to date
        entered = pd.read_sql_query('SELECT DISTINCT analysis, prep_date, ' +
            'fly_num, recording_from FROM responses', conn)

        # TODO more elegant way to check for row w/ certain values?
        curr_entered = (
            (entered.prep_date == date) &
            (entered.fly_num == fly_num) &
            (entered.recording_from == started_at)
        )

        # TODO TODO option to replace older analysis
        if store_multiple_analysis:
            curr_entered = curr_entered & (entered.analysis == analysis_run)

        curr_entered = curr_entered.any()

        # TODO maybe replace analysis w/ same description but earlier version?
        # (where description is just combination of repo names, not w/ version
        # as now
        if curr_entered:
            print('{}, {}, {} already entered with current analysis'.format(
                date, fly_num, thorimage_id))
            continue

        recordings = pd.DataFrame({
            'started_at': [started_at],
            'thorsync_path': [thorsync_dir],
            'thorimage_path': [thorimage_dir],
            'stimulus_data_path': [stimulus_data_path]
        })
        to_sql_with_duplicates(recordings, 'recordings')


        stimfile = recording['stimulus_data_file']
        stimfile_path = join(stimfile_root, stimfile)
        # TODO also err if not readable
        if not os.path.exists(stimfile_path):
            raise ValueError('copy missing stimfile {} to {}'.format(stimfile,
                stimfile_root))

        with open(stimfile_path, 'rb') as f:
            data = pickle.load(f)

        n_repeats = int(data['n_repeats'])

        # The 3 is because 3 odors are compared in each repeat for the
        # natural_odors project.
        presentations_per_repeat = 3

        presentations_per_block = n_repeats * presentations_per_repeat

        n_blocks = int(len(data['odor_pair_list']) / presentations_per_block)

        # TODO TODO subset odor order information by start/end block cols
        # (for natural_odors stuff)
        if pd.isnull(recording['first_block']):
            first_block = 0
        else:
            first_block = int(recording['first_block']) - 1

        if pd.isnull(recording['last_block']):
            last_block = n_blocks - 1
        else:
            last_block = int(recording['last_block']) - 1

        first_presentation = first_block * presentations_per_block
        last_presentation = (last_block + 1) * presentations_per_block

        '''
        print('n_repeats:', n_repeats)
        print('n_blocks:', n_blocks)
        print('first_presentation:', first_presentation)
        print('last_presentation:', last_presentation)
        '''

        # TODO will need to augment w/ concentration info somehow...
        # maybe handle in a way specific to natural_odors project?

        odors = pd.DataFrame({
            'name': data['odors'],
            'log10_conc_vv': [0 if x == 'paraffin' else
                natural_odors_concentrations.at[x,
                'log10_vial_volume_fraction'] for x in data['odors']]
        })

        to_sql_with_duplicates(odors, 'odors')

        # TODO make unique id before insertion? some way that wouldn't require
        # the IDs, but would create similar tables?

        db_odors = pd.read_sql('odors', conn)
        # TODO TODO in general, the name alone won't be unique, so use another
        # strategy
        db_odors.set_index('name', inplace=True)

        # TODO test slicing
        # TODO make sure if there are extra trials in matlab, these get assigned
        # to first
        # + if there are less in matlab, should error
        odor_pair_list = \
            data['odor_pair_list'][first_presentation:last_presentation]

        assert len(odor_pair_list) % (presentations_per_repeat * n_repeats) == 0

        # TODO invert to check
        # TODO is this sql table worth anything if both keys actually need to be
        # referenced later anyway?

        # TODO only add as many as there were blocks from thorsync timing info?
        odor1_ids = [db_odors.at[o1,'odor'] for o1, _ in odor_pair_list]
        odor2_ids = [db_odors.at[o2,'odor'] for _, o2 in odor_pair_list]

        # TODO TODO make unique first. only need order for filling in the values
        # in responses.
        mixtures = pd.DataFrame({
            'odor1': odor1_ids,
            'odor2': odor2_ids
        })

        to_sql_with_duplicates(mixtures, 'mixtures')
        # TODO merge w/ odors to check

        # TODO maybe use Remy's thorsync timing info to get num pulses for prep
        # checking trials and then assume it's ethyl acetate (for PID purposes,
        # at least)?
        # TODO would need to make sure all types are compat to load this way
        #data = evil.load(mat)
        evil.evalc("data = load('{}');".format(mat))
        # sDFF - filtered traces (filtered how?)
        # F0 - background fluorescence (dims?)
        # TODO so what was in sCNM? I guess I don't need it (though
        # maybe h5 couldn't be loaded at all without it?)?
        try:
            S = evil.eval('data.S')
        except matlab.engine.MatlabExecutionError:
            print('CNMF still needs to be run on this data')
            continue
        # TODO rename to indicate this is not just a filtered version of C?
        # (and how exactly is it different again?)
        # Loading w/ h5py lead to transposed indices wrt loading w/ MATLAB
        # engine.
        #filtered_df_over_f = np.array(S['sDFF']).T
        df_over_f = np.array(S['DFF']).T

        # TODO quantitatively compare sDFF / DFF. (s=smoothed)
        # maybe just upload non-smoothed and let people smooth downstream if
        # they want?

        # TODO is this even dF/F'ed? rename in database to indicate?
        # enter as more raw dF/F?
        # TODO TODO TODO actually enter into db!
        raw_f = np.array(evil.eval('data.sCNM.C')).T

        ti = evil.eval('data.ti')
        # TODO dtype appropriate?
        frame_times = np.array(ti['frame_times']).flatten()

        # Frame indices for CNMF output.
        # Of length equal to number of blocks. Each element is the frame
        # index (from 1) in CNMF output that starts the block, where
        # block is defined as a period of continuous acquisition.
        block_first_frames = np.array(ti['trial_start'], dtype=np.uint32
            ).flatten() - 1

        # stim_on is a number as above, but for the frame of the odor
        # onset.
        # TODO how does rounding work here? closest frame? first after?
        odor_onset_frames = np.array(ti['stim_on'], dtype=np.uint32
            ).flatten() - 1
        odor_offset_frames = np.array(ti['stim_off'], dtype=np.uint32
            ).flatten() - 1

        # TODO how to get odor pid
        # A has footprints
        # dims=dimensions of image (256x256)
        # T is # of timestamps
        #print(data['sCNM'])
        #print('')

        # TODO why 474 x 4 + 548 in one case? i thought frame numbers were
        # supposed to be more similar... (w/ np.diff(odor_onset_frames))
        first_onset_frame_offset = odor_onset_frames[0] - block_first_frames[0]

        n_frames, n_cells = df_over_f.shape
        start_frames = np.append(0,
            odor_onset_frames[1:] - first_onset_frame_offset)
        stop_frames = np.append(
            odor_onset_frames[1:] - first_onset_frame_offset - 1, n_frames)
        lens = [stop - start for start, stop in zip(start_frames, stop_frames)]

        # TODO delete version w/ int cast after checking they give same answers
        assert int(frame_times.shape[0]) == int(n_frames)
        assert frame_times.shape[0] == n_frames

        print(start_frames)
        print(stop_frames)
        # TODO find where the discrepancies are!
        print(sum(lens))
        print(n_frames)

        # TODO assert here that all frames add up / approx

        # TODO TODO either warn or err if len(start_frames) is !=
        # len(odor_pair_list)

        odor_id_pairs = [(o1,o2) for o1,o2 in zip(odor1_ids, odor2_ids)]

        comparison_num = -1

        for i in range(len(start_frames)):
            if i % (presentations_per_repeat * n_repeats) == 0:
                comparison_num += 1
                repeat_nums = {id_pair: 0 for id_pair in odor_id_pairs}

            # TODO TODO also save to csv/flat binary/hdf5 per (date, fly,
            # thorimage)
            print('Processing presentation {}'.format(i))

            start_frame = start_frames[i]
            stop_frame = stop_frames[i]
            presentation_dff = df_over_f[start_frame:stop_frame, :]
            presentation_raw_f = raw_f[start_frame:stop_frame, :]

            # TODO off by one?? check
            # TODO check against frames calculated directly from odor offset...
            # may not be const # frames between these "starts" and odor onset?
            onset_frame = start_frame + first_onset_frame_offset

            # TODO check again that these are always equal and delete
            # "direct_onset_frame" bit
            print('onset_frame:', onset_frame)
            direct_onset_frame = odor_onset_frames[i]
            print('direct_onset_frame:', direct_onset_frame)

            # TODO TODO why was i not using direct_onset_frame for this before?
            onset_time = frame_times[direct_onset_frame]
            # TODO check these don't jump around b/c discontinuities
            presentation_frametimes = \
                frame_times[start_frame:stop_frame] - onset_time

            odor_pair = odor_id_pairs[i]
            odor1, odor2 = odor_pair
            repeat_num = repeat_nums[odor_pair]
            repeat_nums[odor_pair] = repeat_num + 1

            offset_frame = odor_offset_frames[i]
            print('offset_frame:', offset_frame)
            assert offset_frame > direct_onset_frame
            # TODO share more of this w/ dataframe creation below, unless that
            # table is changed to just reference presentation table
            presentation = pd.DataFrame({
                'prep_date': [date],
                'fly_num': fly_num,
                'recording_from': started_at,
                'comparison': comparison_num,
                'odor1': odor1,
                'odor2': odor2,
                'repeat_num': repeat_num,
                'odor_onset_frame': direct_onset_frame,
                'odor_offset_frame': offset_frame
            })
            to_sql_with_duplicates(presentation, 'presentations')

            # TODO TODO try inserting timeseries as arrays
            # (insert/read time any shorter then, as opposed to long composite
            #  index?)
            # TODO get remy to save it w/ less than 64 bits of precision?
            # TODO will this automatically make right float index?
            df_over_f_frame = pd.DataFrame(presentation_dff,
                index=presentation_frametimes)

            df_over_f_frame.index.name = 'from_onset'
            df_over_f_frame.columns.name = 'cell'


            raw_f_frame = pd.DataFrame(presentation_raw_f,
                index=presentation_frametimes)

            raw_f_frame.index.name = 'from_onset'
            raw_f_frame.columns.name = 'cell'

              
            presentation_df = pd.melt(
                df_over_f_frame.reset_index(), id_vars='from_onset',
                value_name='df_over_f')

            presentation_df.set_index(['cell','from_onset'],
                inplace=True)

            # TODO TODO merge these two dataframes and insert together, so
            # neither NOT NULL constraint would be violated
            raw_f_frame = pd.melt(
                raw_f_frame.reset_index(), id_vars='from_onset',
                value_name='raw_f')

            raw_f_frame.set_index(['cell','from_onset'],
                inplace=True)

            presentation_df['raw_f'] = raw_f_frame['raw_f']

            metadata = {
                'analysis': analysis_run,
                'prep_date': date,
                'fly_num': fly_num,
                'recording_from': started_at,
                'comparison': comparison_num,
                'odor1': odor1,
                'odor2': odor2,
                'repeat_num': repeat_num
            }
            for k, v in metadata.items():
                presentation_df[k] = v

            # pick particular order here?
            presentation_df.set_index(list(metadata.keys()),
                append=True, inplace=True)

            to_sql_with_duplicates(presentation_df, 'responses',
                index=True)

            print('Done processing presentation {}'.format(i))

        # TODO check that all frames go somewhere and that frames aren't
        # given to two presentations. check they stay w/in block boundaries.
        # (they don't right now. fix!)

# TODO print unused stimfiles / option to delete them

