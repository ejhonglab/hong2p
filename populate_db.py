#!/usr/bin/env python3

"""
Traverses analysis output and loads traces and odor information into database.
"""

import os
from os.path import join, split
import glob
from datetime import datetime
import pickle
import warnings
import atexit
import time
import xml.etree.ElementTree as etree
import pprint

from sqlalchemy import create_engine
import h5py
import numpy as np
import pandas as pd
import matlab.engine
import git


use_cached_gsheet = True
show_inferred_paths = True
convert_h5 = True
calc_timing_info = False
motion_correct = True
only_motion_correct_for_analysis = True


analysis_started_at = time.time()

# TODO need to add stuff to path? what all?
# TODO future have a bug generally, or only if stopped w/ ctrl-d from ipdb like
# i had?
#future = matlab.engine.start_matlab(async=True)
#evil = None
evil = matlab.engine.start_matlab()
atexit.register(evil.quit)

# The latter is here so I can tell which files within it are actually needed
# in the current version of the analysis.
exclude_from_matlab_path = {'CaImAn-MATLAB','matlab_helper_functions'}
#
userpath = evil.userpath()
for root, dirs, _ in os.walk(userpath, topdown=True):
    dirs[:] = [d for d in dirs if (not d.startswith('.') and
        not d.startswith('@') and not d.startswith('+') and
        d not in exclude_from_matlab_path)]

    evil.addpath(root)

# To get Git version information to have a record of what analysis was
# performed.
matlab_repo_name = 'matlab_kc_plane'
matlab_code_path = join(userpath, matlab_repo_name)
#

url = 'postgresql+psycopg2://tracedb:tracedb@localhost:5432/tracedb'
# TODO should this be closed too in atexit?
conn = create_engine(url)


def to_sql_with_duplicates(new_df, table_name, index=False):
    # Other columns should be generated by database anyway.
    cols = list(new_df.columns)
    if index:
        cols += list(new_df.index.names)
    table_cols = ', '.join(cols)

    print('writing to temporary table...')
    new_df.to_sql('temp_' + table_name, conn, if_exists='replace', index=index)

    # TODO change to just get column names?
    query = '''
    SELECT a.attname, format_type(a.atttypid, a.atttypmod) AS data_type
    FROM   pg_index i
    JOIN   pg_attribute a ON a.attrelid = i.indrelid
        AND a.attnum = ANY(i.indkey)
    WHERE  i.indrelid = '{}'::regclass
    AND    i.indisprimary;
    '''.format(table_name)
    result = conn.execute(query)
    pk_cols = ', '.join([n for n, _ in result])

    query = ('INSERT INTO {0} ({1}) SELECT {1} FROM temp_{0} ' +
        'ON CONFLICT ({2}) DO NOTHING').format(table_name, table_cols, pk_cols)
    # TODO let this happen async in the background? (don't need result)
    print('inserting from temporary table... ', end='')
    conn.execute(query)
    print('done')

    # TODO drop staging table


#matlab_output_file = 'test_data/struct_no_sparsearray.mat'
####matlab_output_file = 'test_data/_007_cnmf.mat'

gsheet_cache_file = '.gsheet_cache.p'
if use_cached_gsheet and os.path.exists(gsheet_cache_file):
    print('Loading Google sheet data from cache at {}'.format(
        gsheet_cache_file))

    with open(gsheet_cache_file, 'rb') as f:
        sheets = pickle.load(f)

else:
    with open('google_sheet_link.txt', 'r') as f:
        gsheet_link = f.readline().split('/edit')[0] + '/export?format=csv&gid='

    # If you want to add more sheets, when you select the new sheet in your
    # browser, the GID will be at the end of the URL in the address bar.
    sheet_gids = {
        'fly_preps': '269082112',
        'recordings': '0',
        'daily_settings': '229338960'
    }

    # TODO flag to cache these to just be nice to google?
    sheets = dict()
    for df_name, gid in sheet_gids.items():
        df = pd.read_csv(gsheet_link + gid)

        # TODO convert any other dtypes?
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])

        # TODO complain if there are any missing fly_nums

        sheets[df_name] = df

    boolean_columns = {
        'used_for_analysis',
        'raw_data_discarded',
        'raw_data_lost'
    }
    na_cols = list(set(sheets['recordings'].columns) - boolean_columns)
    sheets['recordings'].dropna(how='all', subset=na_cols, inplace=True)

    with open(gsheet_cache_file, 'wb') as f:
        pickle.dump(sheets, f)

# TODO maybe make df some merge of the three sheets?
df = sheets['recordings']

# TODO say when this happens?
df.drop(df[df.raw_data_discarded].index, inplace=True)
df.drop(df[df.raw_data_lost].index, inplace=True)

keys = ['date', 'fly_num']
df['recording_num'] = df.groupby(keys).cumcount() + 1
# Since otherwise cumcount seems to be zero for stuff without a group...
# (i.e. w/ one of the keys null)
df.loc[pd.isnull(df[keys]).any(axis=1), 'recording_num'] = np.nan

if show_inferred_paths:
    missing_thorimage = pd.isnull(df.thorimage_dir)
    missing_thorsync = pd.isnull(df.thorsync_dir)


df['thorimage_num'] = df.thorimage_dir.apply(lambda x: np.nan if pd.isnull(x)
    else int(x[1:]))
df['numbering_consistent'] = \
    pd.isnull(df.thorimage_num) | (df.thorimage_num == df.recording_num)

# TODO unit test this
# TODO TODO check that, if there are mismatches here, that they *never* happen
# when recording num will be used for inference in rows in the group *after*
# the mismatch
gkeys = keys + ['thorimage_dir','thorsync_dir','thorimage_num',
                'recording_num','numbering_consistent']
for name, group_df in df.groupby(keys):
    '''
    # case 1: all consistent
    # case 2: not all consistent, but all thorimage_dir filled in
    # case 3: not all consistent, but just because thorimage_dir was null
    '''
    #print(group_df[gkeys])

    # TODO check that first_mismatch based approach includes this case
    #if pd.notnull(group_df.thorimage_dir).all():
    #    continue

    mismatches = np.argwhere(~ group_df.numbering_consistent)
    if len(mismatches) == 0:
        continue

    first_mismatch_idx = mismatches[0][0]
    #print('first_mismatch:\n', group_df[gkeys].iloc[first_mismatch_idx])

    # TODO test case where the first mismatch is last
    following_thorimage_dirs = group_df.thorimage_dir.iloc[first_mismatch_idx:]
    #print('checking these are not null:\n', following_thorimage_dirs)
    assert pd.notnull(following_thorimage_dirs).all()

df.thorsync_dir.fillna(df.thorimage_num.apply(lambda x: np.nan if pd.isnull(x)
    else 'SyncData{:03d}'.format(int(x))), inplace=True)

df.drop(columns=['thorimage_num','numbering_consistent'], inplace=True)


# TODO TODO check for conditions in which we might need to renumber recording
# num? (dupes / any entered numbers along the way that are inconsistent w/
# recording_num results)
df.thorimage_dir.fillna(df.recording_num.apply(lambda x: np.nan if pd.isnull(x)
    else'_{:03d}'.format(int(x))), inplace=True)

df.thorsync_dir.fillna(df.recording_num.apply(lambda x: np.nan if pd.isnull(x)
    else 'SyncData{:03d}'.format(int(x))), inplace=True)

if show_inferred_paths:
    cols = ['date','fly_num','thorimage_dir','thorsync_dir']
    print('Inferred ThorImage directories:')
    print(df.loc[missing_thorimage, cols])
    print('\nInferred ThorSync directories:')
    print(df.loc[missing_thorsync, cols])
    print('')

keys = ['date','fly_num']
duped_thorimage = df.duplicated(subset=keys + ['thorimage_dir'], keep=False)
duped_thorsync = df.duplicated(subset=keys + ['thorsync_dir'], keep=False)

try:
    assert not duped_thorimage.any()
    assert not duped_thorsync.any()
except AssertionError:
    print('Duplicated ThorImage directories after path inference:')
    print(df[duped_thorimage])
    print('\nDuplicated ThorSync directories after path inference:')
    print(df[duped_thorsync])
    raise

to_sql_with_duplicates(sheets['fly_preps'].rename(
    columns={'date': 'prep_date'}), 'flies')


# TODO TODO warn if any raw data is not present on NAS / report which
# (could indicate problems w/ path inference)

# TODO add bool col to recordings to indicate whether stimulus order in python
# file was adhered to?
# TODO or just only use it for natural_odors project for now?
# (probably just want to ignore order for project="n/a (prep checking)" columns
# anyway)

# TODO + summarize those that still need analysis run on them (and run it?)
# TODO print stuff w/ used_for_analysis checked w/o either data in database or
# analysis output on disk (or just latter)

# TODO move these paths to config file...
raw_data_root = '/mnt/nas/mb_team/raw_data'
analysis_output_root = '/mnt/nas/mb_team/analysis_output'

rel_to_cnmf_mat = 'cnmf'

stimfile_root = '/mnt/nas/mb_team/stimulus_data_files' 

natural_odors_concentrations = pd.read_csv('natural_odor_panel_vial_concs.csv')
natural_odors_concentrations.set_index('name', inplace=True)
"""

# TODO TODO loop over more stuff than just natural_odors / used_for_analysis
# to load all PID stuff in (will need pin info for prep checking, etc, exps)

# TODO complain if there are flies w/ used_for_analysis not checked w/o
# rejection reason

# TODO maybe don't err in this case (w/ option to only run on analysis?)
# and symmetric option for analysis root?
if not os.path.isdir(raw_data_root):
    raise IOError('raw_data_root {} does not exist'.format(
        raw_data_root))

if not os.path.isdir(analysis_output_root):
    raise IOError('analysis_output_root {} does not exist'.format(
        analysis_output_root))

if not os.path.isdir(stimfile_root):
    raise IOError('stimfile_root {} does not exist'.format(
        stimfile_root))

# TODO TODO also do a first pass over everything w/ default params so just
# manual correction remains

# TODO make fns that take date + fly_num + cwd then re-use iteration
# over date / fly_num (if mirrored data layout for raw / analysis)?

for full_fly_dir in glob.glob(raw_data_root + '/*/*/'):
    full_fly_dir = os.path.normpath(full_fly_dir)
    #print(full_fly_dir)
    prefix, fly_dir = split(full_fly_dir)
    _, date_dir = split(prefix)

    if fly_dir == 'unsorted':
        # TODO maybe make attempt to sort?
        continue

    try:
        fly_num = int(fly_dir)
    except ValueError:
        # TODO maybe warn if not in whitelist, like 'unsorted')
        continue

    try:
        date = datetime.strptime(date_dir, '%Y-%m-%d')
    except ValueError:
        continue

    #print(date)
    #print(fly_num)

    used = df.loc[df.used_for_analysis &
        (df.date == date) & (df.fly_num == fly_num)]

    # TODO maybe do this in analysis actually? (to not just make a bunch of
    # empty dirs...)
    analysis_fly_dir = join(analysis_output_root, date_dir, fly_dir)
    if not os.path.isdir(analysis_fly_dir):
        # Will also make any necessary parent (date) directories.
        os.makedirs(analysis_fly_dir)

    # TODO maybe use regexp to check syncdata / util fn to check for name +
    # stuff in it?
    if convert_h5:
        ####print('Converting ThorSync HDF5 files to .mat...')
        for syncdir in glob.glob(join(full_fly_dir, 'SyncData*')):
            print(syncdir)
            '''
            if evil is None:
                evil = future.result()
                userpath = evil.userpath()
                paths_before = set(evil.path().split(':'))
                for root, dirs, _ in os.walk(userpath, topdown=True):
                    dirs[:] = [d for d in dirs if (not d.startswith('.') and
                        not d.startswith('@') and not d.startswith('+') and
                        d not in exclude_from_matlab_path)]

                    evil.addpath(root)

                paths_after = set(evil.path().split(':'))
                pprint.pprint(paths_after - paths_before)
            '''

            # TODO make it so one ctrl-c closes whole program, rather than just
            # cancelling matlab function and continuing

            # Will immediately return if output already exists.
            evil.thorsync_h5_to_mat(syncdir, full_fly_dir, nargout=0)
            # TODO (check whether she is loosing information... file size really
            # shouldn't be that different. both hdf5...)

            # TODO do i want flags to disable *each* step separately for
            # unanalyzed stuff? just one flag? always ignore that stuff?

            # TODO remy could also convert xml here if she wanted
        ####print('Done converting ThorSync HDF5 to .mat\n')

    if calc_timing_info:
        for _, row in used[['thorimage_dir','thorsync_dir']].iterrows():
            thorimage_dir = join(full_fly_dir, row['thorimage_dir'])
            if not os.path.isdir(thorimage_dir):
                warnings.warn('thorimage_dir {} did not exist for recording ' +
                    'marked as used_for_analysis.')
                continue

            # If not always running h5->mat conversion first, will need to check
            # for the mat, rather than just thorsync_dir.
            thorsync_dir = join(full_fly_dir, row['thorsync_dir'])
            if not os.path.isdir(thorsync_dir):
                warnings.warn('thorsync_dir {} did not exist for recording ' +
                    'marked as used_for_analysis.')
                continue

            print('\nThorImage and ThorSync dirs for get_stiminfo:')
            print(thorimage_dir)
            print(thorsync_dir)
            print('')

            print(('getting stimulus timing information for {}, {}, {}...'
                ).format( date_dir, fly_num, row['thorimage_dir']), end='')

            '''
            # TODO delete
            print('\n')
            print(date_dir)
            print(date_dir == '2019-01-18')
            print(fly_num)
            print(fly_num == 2)
            print(row['thorsync_dir'])
            print(row['thorsync_dir'] == 'SyncData005')
            if not (date_dir == '2019-01-18' and fly_num == 2 and
                #print('SKIPPING KNOWN FAILING GET_STIMINFO CALL')
                print('SKIPPING LIKELY NON-FAILING GET_STIMINFO CALL')
                    row['thorsync_dir'] == 'SyncData005'):
                continue
            #
            '''

            # throwing everything into _<>_cnmf.mat, as we are, would need to
            # inspect it to check whether we already have the stiminfo...
            evil.get_stiminfo(thorimage_dir, row['thorsync_dir'],
                analysis_fly_dir, date_dir, fly_num, nargout=0)

            print(' done.')

    # TODO loop over thorimage dirs and make tifs from each if they don't exist
    # TODO TODO what all metadata is important in the tif? need to script imagej
    # to get most reliable tifs? some python / matlab fn work?
    '''
    for thorimage_dir in glob.glob(join(full_fly_dir, '_*/')):
        thorimage_id = split(os.path.normpath(thorimage_dir))[-1]
    '''

    # maybe avoid searching for thorimage dirs at all if there are no used 
    # rows for this (date,fly) combo, and only_motion_correct_for_analysis

    # TODO use multiple matlab instances to run normcore on different
    # directories in parallel?
    # TODO exclude stuff that indicates it's either already avg or motion
    # corrected? (or just always keep them separately?)
    if motion_correct:
        for input_tif_path in glob.glob(
            join(full_fly_dir, 'tif_stacks', '_*.tif')):

            if only_motion_correct_for_analysis:
                thorimage_id = split(input_tif_path)[-1][:-4]

                recordings = used[used.thorimage_dir == thorimage_id]
                if len(recordings) == 0:
                    continue

            print('\nRunning normcorre_tiff on', input_tif_path)
            # TODO only register one way by default? nonrigid? args to
            # configure?
            evil.normcorre_tiff(input_tif_path, analysis_fly_dir, nargout=0)

    # TODO and if remy wants, copy thorimage xmls

    print('')

    # TODO maybe delete empty folders under analysis?

import sys; sys.exit()
"""

def git_hash(repo_file):
    repo = git.Repo(repo_file, search_parent_directories=True)
    current_hash = repo.head.object.hexsha
    return current_hash
# TODO maybe also return current remote url, if listed? whether pushed?
# TODO store unsaved changes too? sep column? see my metatools package
'''
diff = repo.index.diff(None, create_patch=True)
exactly = 'exactly ' if len(diff) == 0 else ''
'''

this_repo_file = os.path.realpath(__file__)
this_repo_path = split(this_repo_file)[0]
current_hash = git_hash(this_repo_file)
matlab_hash = git_hash(matlab_code_path)

# TODO just store all data separately?
analysis_description = '{}@{}\n{}@{}'.format(this_repo_path, current_hash,
    matlab_code_path, matlab_hash)

# TODO TODO try inserting w/o analysis_run and seeing if serial gets incremented
# otherwise either do it manually or drop that approach


# TODO diff between ** and */ ?
# TODO os.path.join + os invariant way of looping over dirs
for analysis_dir in glob.glob(analysis_output_root+ '/*/*/'):
    analysis_dir = os.path.normpath(analysis_dir)

    prefix, fly_dir = split(analysis_dir)
    _, date_dir = split(prefix)

    try:
        fly_num = int(fly_dir)
    except ValueError:
        continue

    try:
        date = datetime.strptime(date_dir, '%Y-%m-%d')
    except ValueError:
        continue

    print(analysis_dir)

    # TODO TODO complain if stuff marked as used for analysis is not found here
    for mat in glob.glob(join(analysis_dir, rel_to_cnmf_mat, '*_cnmf.mat')):
        prefix = split(mat)[-1].split('_')[:-1]

        print(mat)

        thorimage_id = '_' + prefix[1]

        # TODO TODO need to infer missing thor dirs first, if doing it this
        # way...
        recordings = df.loc[(df.date == date) &
                            (df.thorimage_dir == thorimage_id)]

        if prefix[0] == '':
            # TODO check there is only one fly w/ used_in_analysis checked for
            # this date, then associate that fly num w/ this thorimage id?
            if len(recordings) == 1:
                recording = recordings.iloc[0]
                fly_num = int(recording['fly_num'])

            else:
                # TODO flag to err instead?
                warnings.warn(('{} has no fly_num prefix and the spreadsheet ' +
                    'indicates multiple flies with this date ({}) and ' +
                    'ThorImage directory name ({}). Append proper fly_num' +
                    ' prefix to analysis output.').format(mat, date,
                    thorimage_id))
                continue

        else:
            fly_num = int(prefix[0])
            recordings = recordings.loc[recordings.fly_num == fly_num]
            if len(recordings) > 1:
                # TODO TODO fix case where this can happen if same fly_num
                # has data saved to two different project directories
                # (maybe by just getting rid of project dirs...)
                raise ValueError(('multiple repeats of fly_num {} for ' +
                    '({}, {})').format(fly_num, date, thorimage_id))

            elif len(recordings) == 0:
                raise ValueError(('missing expected fly_num {} for ' +
                    '({}, {})').format(fly_num, date, thorimage_id))

            recording = recordings.iat[0]

        if recording.project != 'natural_odors':
            warnings.warn('project type {} not supported. skipping.')
            continue

        stimfile = recording['stimulus_data_file']
        stimfile_path = join(stimfile_root, stimfile)
        print(stimfile)
        # TODO also err if not readable
        if not os.path.exists(stimfile_path):
            raise ValueError('copy missing stimfile {} to {}'.format(stimfile,
                stimfile_root))

        with open(stimfile_path, 'rb') as f:
            data = pickle.load(f)

        n_repeats = int(data['n_repeats'])

        # The 3 is because 3 odors are compared in each repeat for the
        # natural_odors project.
        presentations_per_repeat = 3

        presentations_per_block = n_repeats * presentations_per_repeat

        n_blocks = int(len(data['odor_pair_list']) / presentations_per_block)

            # TODO TODO subset odor order information by start/end block cols
            # (for natural_odors stuff)
        if pd.isnull(recording['first_block']):
            first_block = 0
        else:
            first_block = int(recording['first_block']) - 1

        if pd.isnull(recording['last_block']):
            last_block = n_blocks - 1
        else:
            last_block = int(recording['last_block']) - 1

        first_presentation = first_block * presentations_per_block
        last_presentation = (last_block + 1) * presentations_per_block

        '''
        print('n_repeats:', n_repeats)
        print('n_blocks:', n_blocks)
        print('first_presentation:', first_presentation)
        print('last_presentation:', last_presentation)
        '''

        # TODO will need to augment w/ concentration info somehow...
        # maybe handle in a way specific to natural_odors project?

        odors = pd.DataFrame({
            'name': data['odors'],
            'log10_conc_vv': [0 if x == 'paraffin' else
                natural_odors_concentrations.at[x,
                'log10_vial_volume_fraction'] for x in data['odors']]
        })

        to_sql_with_duplicates(odors, 'odors')

        # TODO TODO TODO insert and then get ids -> then use that to merge other
        # tables? or what? make unique id before insertion? some way that
        # wouldn't require the IDs, but would create similar tables?

        db_odors = pd.read_sql('odors', conn)
        # TODO TODO in general, the name alone won't be unique, so use another
        # strategy
        db_odors.set_index('name', inplace=True)

        # TODO test slicing
        # TODO make sure if there are extra trials in matlab, these get assigned
        # to first
        # + if there are less in matlab, should error
        odor_pair_list = \
            data['odor_pair_list'][first_presentation:last_presentation]

        assert len(data['odor_pair_list']) % len(odor_pair_list) == 0

        # TODO invert to check
        # TODO is this sql table worth anything if both keys actually need to be
        # referenced later anyway?

        # TODO only add as many as there were blocks from thorsync timing info?
        odor1_ids = [db_odors.at[o1,'odor'] for o1, _ in odor_pair_list]
        odor2_ids = [db_odors.at[o2,'odor'] for _, o2 in odor_pair_list]

        # TODO TODO make unique first. only need order for filling in the values
        # in responses.
        mixtures = pd.DataFrame({
            'odor1': odor1_ids,
            'odor2': odor2_ids
        })

        to_sql_with_duplicates(mixtures, 'mixtures')
        # TODO merge w/ odors to check

        analyzed_at = datetime.fromtimestamp(analysis_started_at)

        analysis_runs = pd.DataFrame({
            'analysis_description': [analysis_description],
            'analyzed_at': [analyzed_at]
        })
        # Run start should always change, so we don't need to worry about
        # duplicates on insertion here.
        analysis_runs.to_sql('analysis_runs', conn, if_exists='append',
            index=False)
        # TODO TODO prevent timestamp conversion that seems to be happening in
        # database insert

        # Need to do this as long as the part of the key indicating the
        # analysis, in the recordings table, is generated by the database.
        db_analysis_runs = pd.read_sql('analysis_runs', conn).set_index(
            ['analysis_description', 'analyzed_at'])
        analysis_run = \
            db_analysis_runs.loc[(analysis_description, analyzed_at)][0]


        raw_fly_dir = join(raw_data_root, date_dir, fly_dir)
        thorsync_dir = join(raw_fly_dir, recording['thorsync_dir'])
        thorimage_dir = join(raw_fly_dir, recording['thorimage_dir'])
        stimulus_data_path = join(stimfile_root,
                                  recording['stimulus_data_file'])

        # TODO for recordings.started_at, load time from one of the thorlabs
        # files
        # TODO check that ThorImageExperiment/Date/uTime parses to date field,
        # w/ appropriate timezone settings (or insert raw and check database has
        # something like date)?
        thorimage_xml_path = join(thorimage_dir, 'Experiment.xml')
        xml_root = etree.parse(thorimage_xml_path).getroot()
        started_at = \
            datetime.fromtimestamp(float(xml_root.find('Date').attrib['uTime']))

        recordings = pd.DataFrame({
            'started_at': [started_at],
            'thorsync_path': [thorsync_dir],
            'thorimage_path': [thorimage_dir],
            'stimulus_data_path': [stimulus_data_path]
        })
        to_sql_with_duplicates(recordings, 'recordings')

        '''
        pulse_length = 1.0
        # TODO get from daily_settings table
        odor_flow_slpm = 0.4
        carrier_flow_slpm = 1.6
        #
        volume_ml = 2.0
        '''

        # TODO maybe use Remy's thorsync timing info to get num pulses for prep
        # checking trials and then assume it's ethyl acetate (for PID purposes,
        # at least)?

        with h5py.File(mat, 'r') as data:
            # TODO delete?
            if 'sCNM' not in data.keys():
                warnings.warn('no sCNM object in {}'.format(mat))
                continue

            ti = data['ti']
            # TODO dtype appropriate?
            frame_times = np.array(ti['frame_times']).flatten()

            # Frame indices for CNMF output.
            # Of length equal to number of blocks. Each element is the frame
            # index (from 1) in CNMF output that starts the block, where block
            # is defined as a period of continuous acquisition.
            block_first_frames = np.array(ti['trial_start'], dtype=np.uint32
                ).flatten() - 1

            # stim_on is a number as above, but for the frame of the odor onset.
            # TODO how does rounding work here? closest frame? first after?
            odor_onset_frames = np.array(ti['stim_on'], dtype=np.uint32
                ).flatten() - 1

            # TODO how to get odor pid
            # A has footprints
            # dims=dimensions of image (256x256)
            # T is # of timestamps
            #print(data['sCNM'])
            #print('')

            # DFF - traces
            # sDFF - filtered traces (filtered how?)
            # F0 - background fluorescence (dims?)
            # TODO so what was in sCNM? I guess I don't need it (though maybe h5
            # couldn't be loaded at all without it?)?
            traces = np.array(data['S']['sDFF'])

        # TODO why 474 x 4 + 548 in one case? i thought frame numbers were
        # supposed to be more similar... (w/ np.diff(odor_onset_frames))
        first_onset_frame_offset = odor_onset_frames[0] - block_first_frames[0]

        n_frames, n_cells = traces.shape
        start_frames = np.append(0,
            odor_onset_frames[1:] - first_onset_frame_offset)
        stop_frames = np.append(
            odor_onset_frames[1:] - first_onset_frame_offset - 1, n_frames)
        lens = [stop - start for start, stop in zip(start_frames, stop_frames)]

        # TODO find where the discrepancies are!
        print(sum(lens))
        print(n_frames)

        # TODO assert here that all frames add up / approx

        # TODO TODO either warn or err if len(start_frames) is !=
        # len(odor_pair_list)

        print(start_frames)
        print(stop_frames)

        odor_id_pairs = [(o1,o2) for o1,o2 in zip(odor1_ids, odor2_ids)]
        repeat_nums = {id_pair: 0 for id_pair in odor_id_pairs}

        for i in range(len(start_frames)):
            start_frame = start_frames[i]
            stop_frame = stop_frames[i]
            presentation_traces = traces[start_frame:stop_frame, :]

            onset_time = frame_times[start_frame + first_onset_frame_offset]
            # TODO check these don't jump around b/c discontinuities
            presentation_frametimes = \
                frame_times[start_frame:stop_frame] - onset_time

            odor_pair = odor_id_pairs[i]
            odor1, odor2 = odor_pair
            repeat_num = repeat_nums[odor_pair]
            repeat_nums[odor_pair] = repeat_num + 1

            # TODO TODO try inserting timeseries as arrays
            # (insert/read time any shorter then, as opposed to long composite
            #  index?)
            # TODO get remy to save it w/ less than 64 bits of precision?
            # TODO will this automatically make right float index?
            trace_df = pd.DataFrame(presentation_traces,
                                    index=presentation_frametimes)
            trace_df.index.name = 'from_onset'
            trace_df.columns.name = 'cell'

            trace_df = pd.melt(trace_df.reset_index(), id_vars='from_onset',
                value_name='df_over_f')

            metadata = {
                'analysis': analysis_run,
                'prep_date': date,
                'fly_num': fly_num,
                'recording_from': started_at,
                'odor1': odor1,
                'odor2': odor2,
                'repeat_num': repeat_num
            }
            for k, v in metadata.items():
                trace_df[k] = v

            # pick particular order here?
            trace_df.set_index(list(metadata.keys()), inplace=True)
            trace_df.set_index(['cell','from_onset'], append=True, inplace=True)

            to_sql_with_duplicates(trace_df, 'responses', index=True)

        # TODO check that all frames go somewhere and that frames aren't
        # given to two presentations. check they stay w/in block boundaries.
        # (they don't right now. fix!)

        import ipdb; ipdb.set_trace()

# TODO print unused stimfiles / option to delete them

