#!/usr/bin/env python3

"""
Traverses analysis output and loads traces and odor information into database.
"""

import os
from os.path import join, split
import glob
from datetime import datetime
import pickle
import warnings
import atexit
import time
import xml.etree.ElementTree as etree
import pprint

from sqlalchemy import create_engine, MetaData, Table
# TODO or just use sqlalchemy.types?
from sqlalchemy.sql import sqltypes
from sqlalchemy.dialects import postgresql
import h5py
import numpy as np
from scipy.sparse import coo_matrix
import pandas as pd
import matlab.engine
import git


verbose = False

use_cached_gsheet = True
show_inferred_paths = True
convert_h5 = True
calc_timing_info = False
motion_correct = True
only_motion_correct_for_analysis = True

# TODO make sure that incomplete entries are not preventing full
# analysis from being inserted, despite setting of this flag
overwrite_older_analysis = True


analysis_started_at = time.time()

# TODO need to add stuff to path? what all?
# TODO future have a bug generally, or only if stopped w/ ctrl-d from ipdb like
# i had?
#future = matlab.engine.start_matlab(async=True)
#evil = None
evil = matlab.engine.start_matlab()
atexit.register(evil.quit)

# The latter is here so I can tell which files within it are actually needed
# in the current version of the analysis.
exclude_from_matlab_path = {'CaImAn-MATLAB','matlab_helper_functions'}
#
userpath = evil.userpath()
for root, dirs, _ in os.walk(userpath, topdown=True):
    dirs[:] = [d for d in dirs if (not d.startswith('.') and
        not d.startswith('@') and not d.startswith('+') and
        d not in exclude_from_matlab_path)]

    evil.addpath(root)

# To get Git version information to have a record of what analysis was
# performed.
matlab_repo_name = 'matlab_kc_plane'
matlab_code_path = join(userpath, matlab_repo_name)
#

url = 'postgresql+psycopg2://tracedb:tracedb@localhost:5432/tracedb'
# TODO should this be closed too in atexit?
conn = create_engine(url)


def to_sql_with_duplicates(new_df, table_name, index=False, verbose=False):
    # TODO TODO if this fails and time won't be saved on reinsertion, any rows
    # that have been inserted already should be deleted to avoid confusion
    # (mainly, for the case where the program is interrupted while this is
    # running)
    # TODO TODO maybe have some cleaning step that checks everything in database
    # has the correct number of rows? and maybe prompts to delete?

    # Other columns should be generated by database anyway.
    cols = list(new_df.columns)
    if index:
        cols += list(new_df.index.names)
    table_cols = ', '.join(cols)

    md = MetaData()
    table = Table(table_name, md, autoload_with=conn)
    dtypes = {c.name: c.type for c in table.c}

    if verbose:
        print('SQL column types:')
        pprint.pprint(dtypes)
   
    df_types = new_df.dtypes.to_dict()
    if index:
        df_types.update({n: new_df.index.get_level_values(n).dtype
            for n in new_df.index.names})

    if verbose:
        print('\nOld dataframe column types:')
        pprint.pprint(df_types)

    sqlalchemy2pd_type = {
        'INTEGER()': np.dtype('int32'),
        'SMALLINT()': np.dtype('int16'),
        'REAL()': np.dtype('float32'),
        'DOUBLE_PRECISION(precision=53)': np.dtype('float64'),
        # TODO but maybe this was the type causing the problem?
        'DATE()': np.dtype('<M8[ns]')
    }
    if verbose:
        print('\nSQL types to cast:')
        pprint.pprint(sqlalchemy2pd_type)

    new_df_types = {n: sqlalchemy2pd_type[repr(t)] for n, t in dtypes.items()
        if repr(t) in sqlalchemy2pd_type}

    if verbose:
        print('\nNew dataframe column types:')
        pprint.pprint(new_df_types)

    # TODO how to get around converting things to int if they have NaN.
    # possible to not convert?
    new_column_types = dict()
    new_index_types = dict()
    for k, t in new_df_types.items():
        if k in new_df.columns and not new_df[k].isnull().any():
            new_column_types[k] = t

        # TODO or is it always true that index level can't be NaN anyway?
        elif (k in new_df.index.names and
            not new_df.index.get_level_values(k).isnull().any()):

            new_index_types[k] = t

        # TODO print types being skipped b/c nan?

    new_df = new_df.astype(new_column_types, copy=False)
    if index:
        # TODO need to handle case where conversion dict is empty
        # (seems to fail?)
        #pprint.pprint(new_index_types)

        # MultiIndex astype method seems to not work the same way?
        new_df.index = pd.MultiIndex.from_frame(
            new_df.index.to_frame().astype(new_index_types, copy=False))

    # TODO print the type of any sql types not convertible?
    # TODO assert all dtypes can be converted w/ this dict?

    if index:
        print('writing to temporary table temp_{}...'.format(table_name))

    # TODO figure out how to profile
    new_df.to_sql('temp_' + table_name, conn, if_exists='replace', index=index,
        dtype=dtypes)

    # TODO change to just get column names?
    query = '''
    SELECT a.attname, format_type(a.atttypid, a.atttypmod) AS data_type
    FROM   pg_index i
    JOIN   pg_attribute a ON a.attrelid = i.indrelid
        AND a.attnum = ANY(i.indkey)
    WHERE  i.indrelid = '{}'::regclass
    AND    i.indisprimary;
    '''.format(table_name)
    result = conn.execute(query)
    pk_cols = ', '.join([n for n, _ in result])

    # TODO TODO TODO modify so on conflict the new row replaces the old one!
    # (for updates to analysis, if exact code version w/ uncommited changes and
    # everything is not going to be part of primary key...)
    # (want updates to non-PK rows)
    # TODO prefix w/ ANALYZE EXAMINE and look at results
    query = ('INSERT INTO {0} ({1}) SELECT {1} FROM temp_{0} ' +
        'ON CONFLICT ({2}) DO NOTHING').format(table_name, table_cols, pk_cols)
    # TODO maybe a merge is better for this kind of upsert, in postgres?
    if index:
        print('inserting into {} from temporary table... '.format(table_name),
            end='')

    # TODO let this happen async in the background? (don't need result)
    conn.execute(query)

    if index:
        print('done')

    # TODO drop staging table


gsheet_cache_file = '.gsheet_cache.p'
if use_cached_gsheet and os.path.exists(gsheet_cache_file):
    print('Loading Google sheet data from cache at {}'.format(
        gsheet_cache_file))

    with open(gsheet_cache_file, 'rb') as f:
        sheets = pickle.load(f)

else:
    with open('google_sheet_link.txt', 'r') as f:
        gsheet_link = f.readline().split('/edit')[0] + '/export?format=csv&gid='

    # If you want to add more sheets, when you select the new sheet in your
    # browser, the GID will be at the end of the URL in the address bar.
    sheet_gids = {
        'fly_preps': '269082112',
        'recordings': '0',
        'daily_settings': '229338960'
    }

    # TODO flag to cache these to just be nice to google?
    sheets = dict()
    for df_name, gid in sheet_gids.items():
        df = pd.read_csv(gsheet_link + gid)

        # TODO convert any other dtypes?
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])

        # TODO complain if there are any missing fly_nums

        sheets[df_name] = df

    boolean_columns = {
        'used_for_analysis',
        'raw_data_discarded',
        'raw_data_lost'
    }
    na_cols = list(set(sheets['recordings'].columns) - boolean_columns)
    sheets['recordings'].dropna(how='all', subset=na_cols, inplace=True)

    with open(gsheet_cache_file, 'wb') as f:
        pickle.dump(sheets, f)

# TODO maybe make df some merge of the three sheets?
df = sheets['recordings']

df.raw_data_discarded = df.raw_data_discarded.fillna(False)
# TODO say when this happens?
df.drop(df[df.raw_data_discarded].index, inplace=True)

# TODO TODO warn / fail if 'used_for_analysis' and either discard / lost is
# checked

# Not sure where there were any NaN here anyway...
df.raw_data_lost = df.raw_data_lost.fillna(False)
df.drop(df[df.raw_data_lost].index, inplace=True)

keys = ['date', 'fly_num']
df['recording_num'] = df.groupby(keys).cumcount() + 1
# Since otherwise cumcount seems to be zero for stuff without a group...
# (i.e. w/ one of the keys null)
df.loc[pd.isnull(df[keys]).any(axis=1), 'recording_num'] = np.nan

# TODO delete hack after dealing w/ remy's conventions (some of which were
# breaking the code assuming my conventions)
df.drop(df[df.project != 'natural_odors'].index, inplace=True)

if show_inferred_paths:
    missing_thorimage = pd.isnull(df.thorimage_dir)
    missing_thorsync = pd.isnull(df.thorsync_dir)


df['thorimage_num'] = df.thorimage_dir.apply(lambda x: np.nan if pd.isnull(x)
    else int(x[1:]))
df['numbering_consistent'] = \
    pd.isnull(df.thorimage_num) | (df.thorimage_num == df.recording_num)

# TODO unit test this
# TODO TODO check that, if there are mismatches here, that they *never* happen
# when recording num will be used for inference in rows in the group *after*
# the mismatch
gkeys = keys + ['thorimage_dir','thorsync_dir','thorimage_num',
                'recording_num','numbering_consistent']
for name, group_df in df.groupby(keys):
    '''
    # case 1: all consistent
    # case 2: not all consistent, but all thorimage_dir filled in
    # case 3: not all consistent, but just because thorimage_dir was null
    '''
    #print(group_df[gkeys])

    # TODO check that first_mismatch based approach includes this case
    #if pd.notnull(group_df.thorimage_dir).all():
    #    continue

    mismatches = np.argwhere(~ group_df.numbering_consistent)
    if len(mismatches) == 0:
        continue

    first_mismatch_idx = mismatches[0][0]
    #print('first_mismatch:\n', group_df[gkeys].iloc[first_mismatch_idx])

    # TODO test case where the first mismatch is last
    following_thorimage_dirs = group_df.thorimage_dir.iloc[first_mismatch_idx:]
    #print('checking these are not null:\n', following_thorimage_dirs)
    assert pd.notnull(following_thorimage_dirs).all()

df.thorsync_dir.fillna(df.thorimage_num.apply(lambda x: np.nan if pd.isnull(x)
    else 'SyncData{:03d}'.format(int(x))), inplace=True)

df.drop(columns=['thorimage_num','numbering_consistent'], inplace=True)


# TODO TODO check for conditions in which we might need to renumber recording
# num? (dupes / any entered numbers along the way that are inconsistent w/
# recording_num results)
df.thorimage_dir.fillna(df.recording_num.apply(lambda x: np.nan if pd.isnull(x)
    else'_{:03d}'.format(int(x))), inplace=True)

df.thorsync_dir.fillna(df.recording_num.apply(lambda x: np.nan if pd.isnull(x)
    else 'SyncData{:03d}'.format(int(x))), inplace=True)

if show_inferred_paths:
    cols = ['date','fly_num','thorimage_dir','thorsync_dir']
    print('Inferred ThorImage directories:')
    print(df.loc[missing_thorimage, cols])
    print('\nInferred ThorSync directories:')
    print(df.loc[missing_thorsync, cols])
    print('')

keys = ['date','fly_num']
duped_thorimage = df.duplicated(subset=keys + ['thorimage_dir'], keep=False)
duped_thorsync = df.duplicated(subset=keys + ['thorsync_dir'], keep=False)

try:
    assert not duped_thorimage.any()
    assert not duped_thorsync.any()
except AssertionError:
    print('Duplicated ThorImage directories after path inference:')
    print(df[duped_thorimage])
    print('\nDuplicated ThorSync directories after path inference:')
    print(df[duped_thorsync])
    raise

sheets['fly_preps'].dropna(subset=['date','fly_num'], inplace=True)
to_sql_with_duplicates(sheets['fly_preps'].rename(
    columns={'date': 'prep_date'}), 'flies')


# TODO TODO warn if any raw data is not present on NAS / report which
# (could indicate problems w/ path inference)

# TODO add bool col to recordings to indicate whether stimulus order in python
# file was adhered to?
# TODO or just only use it for natural_odors project for now?
# (probably just want to ignore order for project="n/a (prep checking)" columns
# anyway)

# TODO + summarize those that still need analysis run on them (and run it?)
# TODO print stuff w/ used_for_analysis checked w/o either data in database or
# analysis output on disk (or just latter)

# TODO move these paths to config file...
raw_data_root = '/mnt/nas/mb_team/raw_data'
analysis_output_root = '/mnt/nas/mb_team/analysis_output'

rel_to_cnmf_mat = 'cnmf'

stimfile_root = '/mnt/nas/mb_team/stimulus_data_files' 

natural_odors_concentrations = pd.read_csv('natural_odor_panel_vial_concs.csv')
natural_odors_concentrations.set_index('name', inplace=True)

# TODO TODO loop over more stuff than just natural_odors / used_for_analysis
# to load all PID stuff in (will need pin info for prep checking, etc, exps)

# TODO complain if there are flies w/ used_for_analysis not checked w/o
# rejection reason

# TODO maybe don't err in this case (w/ option to only run on analysis?)
# and symmetric option for analysis root?
if not os.path.isdir(raw_data_root):
    raise IOError('raw_data_root {} does not exist'.format(
        raw_data_root))

if not os.path.isdir(analysis_output_root):
    raise IOError('analysis_output_root {} does not exist'.format(
        analysis_output_root))

if not os.path.isdir(stimfile_root):
    raise IOError('stimfile_root {} does not exist'.format(
        stimfile_root))

# TODO TODO also do a first pass over everything w/ default params so just
# manual correction remains

# TODO make fns that take date + fly_num + cwd then re-use iteration
# over date / fly_num (if mirrored data layout for raw / analysis)?

for full_fly_dir in glob.glob(raw_data_root + '/*/*/'):
    full_fly_dir = os.path.normpath(full_fly_dir)
    #print(full_fly_dir)
    prefix, fly_dir = split(full_fly_dir)
    _, date_dir = split(prefix)

    if fly_dir == 'unsorted':
        # TODO maybe make attempt to sort?
        continue

    try:
        fly_num = int(fly_dir)
    except ValueError:
        # TODO maybe warn if not in whitelist, like 'unsorted')
        continue

    try:
        date = datetime.strptime(date_dir, '%Y-%m-%d')
    except ValueError:
        continue

    print('Date:', date_dir)
    print('Fly:', fly_num)

    used = df.loc[df.used_for_analysis &
        (df.date == date) & (df.fly_num == fly_num)]

    if len(used) > 0:
        print('Used ThorImage dirs:')
        for d in used.thorimage_dir:
            print(d)
    else:
        print('No used ThorImage dirs.')

    # TODO maybe do this in analysis actually? (to not just make a bunch of
    # empty dirs...)
    analysis_fly_dir = join(analysis_output_root, date_dir, fly_dir)
    if not os.path.isdir(analysis_fly_dir):
        # Will also make any necessary parent (date) directories.
        os.makedirs(analysis_fly_dir)

    # TODO maybe use regexp to check syncdata / util fn to check for name +
    # stuff in it?
    if convert_h5:
        ####print('Converting ThorSync HDF5 files to .mat...')
        for syncdir in glob.glob(join(full_fly_dir, 'SyncData*')):
            #print(syncdir)
            '''
            if evil is None:
                evil = future.result()
                userpath = evil.userpath()
                paths_before = set(evil.path().split(':'))
                for root, dirs, _ in os.walk(userpath, topdown=True):
                    dirs[:] = [d for d in dirs if (not d.startswith('.') and
                        not d.startswith('@') and not d.startswith('+') and
                        d not in exclude_from_matlab_path)]

                    evil.addpath(root)

                paths_after = set(evil.path().split(':'))
                pprint.pprint(paths_after - paths_before)
            '''

            # TODO make it so one ctrl-c closes whole program, rather than just
            # cancelling matlab function and continuing

            # Will immediately return if output already exists.
            evil.thorsync_h5_to_mat(syncdir, full_fly_dir, nargout=0)
            # TODO (check whether she is loosing information... file size really
            # shouldn't be that different. both hdf5...)

            # TODO do i want flags to disable *each* step separately for
            # unanalyzed stuff? just one flag? always ignore that stuff?

            # TODO remy could also convert xml here if she wanted
        ####print('Done converting ThorSync HDF5 to .mat\n')

    if calc_timing_info:
        for _, row in used[['thorimage_dir','thorsync_dir']].iterrows():
            thorimage_dir = join(full_fly_dir, row['thorimage_dir'])
            if not os.path.isdir(thorimage_dir):
                warnings.warn('thorimage_dir {} did not exist for recording ' +
                    'marked as used_for_analysis.')
                continue

            # If not always running h5->mat conversion first, will need to check
            # for the mat, rather than just thorsync_dir.
            thorsync_dir = join(full_fly_dir, row['thorsync_dir'])
            if not os.path.isdir(thorsync_dir):
                warnings.warn('thorsync_dir {} did not exist for recording ' +
                    'marked as used_for_analysis.')
                continue

            print('\nThorImage and ThorSync dirs for get_stiminfo:')
            print(thorimage_dir)
            print(thorsync_dir)
            print('')

            print(('getting stimulus timing information for {}, {}, {}...'
                ).format( date_dir, fly_num, row['thorimage_dir']), end='')

            '''
            # TODO delete
            print('\n')
            print(date_dir)
            print(date_dir == '2019-01-18')
            print(fly_num)
            print(fly_num == 2)
            print(row['thorsync_dir'])
            print(row['thorsync_dir'] == 'SyncData005')
            if not (date_dir == '2019-01-18' and fly_num == 2 and
                #print('SKIPPING KNOWN FAILING GET_STIMINFO CALL')
                print('SKIPPING LIKELY NON-FAILING GET_STIMINFO CALL')
                    row['thorsync_dir'] == 'SyncData005'):
                continue
            #
            '''

            # throwing everything into _<>_cnmf.mat, as we are, would need to
            # inspect it to check whether we already have the stiminfo...
            evil.get_stiminfo(thorimage_dir, row['thorsync_dir'],
                analysis_fly_dir, date_dir, fly_num, nargout=0)

            print(' done.')

    # TODO loop over thorimage dirs and make tifs from each if they don't exist
    # TODO TODO what all metadata is important in the tif? need to script imagej
    # to get most reliable tifs? some python / matlab fn work?
    '''
    for thorimage_dir in glob.glob(join(full_fly_dir, '_*/')):
        thorimage_id = split(os.path.normpath(thorimage_dir))[-1]
    '''

    # maybe avoid searching for thorimage dirs at all if there are no used 
    # rows for this (date,fly) combo, and only_motion_correct_for_analysis

    # TODO use multiple matlab instances to run normcore on different
    # directories in parallel?
    # TODO exclude stuff that indicates it's either already avg or motion
    # corrected? (or just always keep them separately?)
    if motion_correct:
        # TODO maybe also look w/o underscore, if that's remy's convention
        for input_tif_path in glob.glob(
            join(full_fly_dir, 'tif_stacks', '_*.tif')):

            if only_motion_correct_for_analysis:
                thorimage_id = split(input_tif_path)[-1][:-4]

                recordings = used[used.thorimage_dir == thorimage_id]
                if len(recordings) == 0:
                    continue

            print('\nRunning normcorre_tiff on', input_tif_path)
            # TODO only register one way by default? nonrigid? args to
            # configure?
            evil.normcorre_tiff(input_tif_path, analysis_fly_dir, nargout=0)
            #import ipdb; ipdb.set_trace()

    # TODO and if remy wants, copy thorimage xmls

    print('')

    # TODO maybe delete empty folders under analysis? (do in atexit handler)

#import sys; sys.exit()

def git_hash(repo_file):
    repo = git.Repo(repo_file, search_parent_directories=True)
    current_hash = repo.head.object.hexsha
    return current_hash
# TODO maybe also return current remote url, if listed? whether pushed?
# TODO store unsaved changes too? sep column? see my metatools package
'''
diff = repo.index.diff(None, create_patch=True)
exactly = 'exactly ' if len(diff) == 0 else ''
'''

this_repo_file = os.path.realpath(__file__)
this_repo_path = split(this_repo_file)[0]
current_hash = git_hash(this_repo_file)
matlab_hash = git_hash(matlab_code_path)

# TODO just store all data separately?
# TODO TODO maybe just use matlab code repo + description in analysis
# description that gets checked? (because that's what actually generates cnmf,
# which is used for responses)
analysis_description = '{}@{}\n{}@{}'.format(this_repo_path, current_hash,
    matlab_code_path, matlab_hash)

analyzed_at = datetime.fromtimestamp(analysis_started_at)

# TODO TODO clean this of runs that don't have data in the database...
# (+ reindex serial id?)
analysis_runs = pd.DataFrame({
    'analysis_description': [analysis_description],
    'analyzed_at': [analyzed_at]
})
# TODO don't insert into this if dependent stuff won't be written? same for some
# of the other metadata tables?
to_sql_with_duplicates(analysis_runs, 'analysis_runs')

# Need to do this as long as the part of the key indicating the
# analysis, in the recordings table, is generated by the database.
db_analysis_runs = pd.read_sql('analysis_runs', conn).set_index(
    'analysis_description')
analysis_run = \
    db_analysis_runs.loc[analysis_description, 'analysis_run']


# TODO diff between ** and */ ?
# TODO os.path.join + os invariant way of looping over dirs
for analysis_dir in glob.glob(analysis_output_root+ '/*/*/'):
    analysis_dir = os.path.normpath(analysis_dir)

    prefix, fly_dir = split(analysis_dir)
    _, date_dir = split(prefix)

    try:
        fly_num = int(fly_dir)
    except ValueError:
        continue

    try:
        date = datetime.strptime(date_dir, '%Y-%m-%d')
    except ValueError:
        continue


    mat_files = glob.glob(join(analysis_dir, rel_to_cnmf_mat, '*_cnmf.mat'))
    # TODO both in this case and w/ stuff above, maybe don't print anything in
    # case where no data is found
    if len(mat_files) == 0:
        if verbose:
            print(analysis_dir)
            print('no CNMF output MAT files')
        continue

    # TODO complain if stuff marked as used for analysis is not found here
    for mat in mat_files:
        print(mat)

        prefix = split(mat)[-1].split('_')[:-1]

        thorimage_id = '_' + prefix[1]
        '''
        # TODO TODO delete
        # it doesn't even seem there was a _002 here? which was causing some
        # problem?
        if not (date_dir == '2019-01-18' and fly_num == 2 and
                thorimage_id == '_002'):
            print('skipping')
            continue
        print('not skipping this one')
        #
        '''

        recordings = df.loc[(df.date == date) & (df.fly_num == fly_num) &
                            (df.thorimage_dir == thorimage_id)]
        recording = recordings.iloc[0]

        if recording.project != 'natural_odors':
            warnings.warn('project type {} not supported. skipping.')
            continue


        raw_fly_dir = join(raw_data_root, date_dir, fly_dir)
        thorsync_dir = join(raw_fly_dir, recording['thorsync_dir'])
        thorimage_dir = join(raw_fly_dir, recording['thorimage_dir'])
        stimulus_data_path = join(stimfile_root,
                                  recording['stimulus_data_file'])

        # TODO for recordings.started_at, load time from one of the thorlabs
        # files
        # TODO check that ThorImageExperiment/Date/uTime parses to date field,
        # w/ appropriate timezone settings (or insert raw and check database has
        # something like date)?
        thorimage_xml_path = join(thorimage_dir, 'Experiment.xml')
        xml_root = etree.parse(thorimage_xml_path).getroot()
        started_at = \
            datetime.fromtimestamp(float(xml_root.find('Date').attrib['uTime']))


        # TODO check this whole section acter analysis refactoring...
        entered = pd.read_sql_query('SELECT DISTINCT prep_date, ' +
            'fly_num, recording_from, analysis FROM presentations', conn)
        # TODO TODO check that the right number of rows are in there, otherwise
        # drop and re-insert (optionally? since it might take a bit of time to
        # load CNMF output to check / for database to check)

        # TODO more elegant way to check for row w/ certain values?
        curr_entered = (
            (entered.prep_date == date) &
            (entered.fly_num == fly_num) &
            (entered.recording_from == started_at)
        )

        if overwrite_older_analysis:
            curr_entered = curr_entered & (entered.analysis == analysis_run)

        curr_entered = curr_entered.any()

        # TODO maybe replace analysis w/ same description but earlier version?
        # (where description is just combination of repo names, not w/ version
        # as now
        if curr_entered:
            print('{}, {}, {} already entered with current analysis'.format(
                date, fly_num, thorimage_id))
            continue

        recordings = pd.DataFrame({
            'started_at': [started_at],
            'thorsync_path': [thorsync_dir],
            'thorimage_path': [thorimage_dir],
            'stimulus_data_path': [stimulus_data_path]
        })
        to_sql_with_duplicates(recordings, 'recordings')


        stimfile = recording['stimulus_data_file']
        stimfile_path = join(stimfile_root, stimfile)
        # TODO also err if not readable
        if not os.path.exists(stimfile_path):
            raise ValueError('copy missing stimfile {} to {}'.format(stimfile,
                stimfile_root))

        with open(stimfile_path, 'rb') as f:
            data = pickle.load(f)

        n_repeats = int(data['n_repeats'])

        # The 3 is because 3 odors are compared in each repeat for the
        # natural_odors project.
        presentations_per_repeat = 3

        presentations_per_block = n_repeats * presentations_per_repeat

        n_blocks = int(len(data['odor_pair_list']) / presentations_per_block)

        # TODO TODO subset odor order information by start/end block cols
        # (for natural_odors stuff)
        if pd.isnull(recording['first_block']):
            first_block = 0
        else:
            first_block = int(recording['first_block']) - 1

        if pd.isnull(recording['last_block']):
            last_block = n_blocks - 1
        else:
            last_block = int(recording['last_block']) - 1

        first_presentation = first_block * presentations_per_block
        last_presentation = (last_block + 1) * presentations_per_block

        '''
        print('n_repeats:', n_repeats)
        print('n_blocks:', n_blocks)
        print('first_presentation:', first_presentation)
        print('last_presentation:', last_presentation)
        '''

        # TODO will need to augment w/ concentration info somehow...
        # maybe handle in a way specific to natural_odors project?

        odors = pd.DataFrame({
            'name': data['odors'],
            'log10_conc_vv': [0 if x == 'paraffin' else
                natural_odors_concentrations.at[x,
                'log10_vial_volume_fraction'] for x in data['odors']]
        })

        to_sql_with_duplicates(odors, 'odors')

        # TODO make unique id before insertion? some way that wouldn't require
        # the IDs, but would create similar tables?

        db_odors = pd.read_sql('odors', conn)
        # TODO TODO in general, the name alone won't be unique, so use another
        # strategy
        db_odors.set_index('name', inplace=True)

        # TODO test slicing
        # TODO make sure if there are extra trials in matlab, these get assigned
        # to first
        # + if there are less in matlab, should error
        odor_pair_list = \
            data['odor_pair_list'][first_presentation:last_presentation]

        assert len(odor_pair_list) % (presentations_per_repeat * n_repeats) == 0

        # TODO invert to check
        # TODO is this sql table worth anything if both keys actually need to be
        # referenced later anyway?

        # TODO only add as many as there were blocks from thorsync timing info?
        odor1_ids = [db_odors.at[o1,'odor'] for o1, _ in odor_pair_list]
        odor2_ids = [db_odors.at[o2,'odor'] for _, o2 in odor_pair_list]

        # TODO TODO make unique first. only need order for filling in the values
        # in responses.
        mixtures = pd.DataFrame({
            'odor1': odor1_ids,
            'odor2': odor2_ids
        })

        to_sql_with_duplicates(mixtures, 'mixtures')
        # TODO merge w/ odors to check

        # TODO maybe use Remy's thorsync timing info to get num pulses for prep
        # checking trials and then assume it's ethyl acetate (for PID purposes,
        # at least)?
        # TODO would need to make sure all types are compat to load this way
        #print('loading MAT file in MATLAB...', end='')

        evil.evalc("clear; data = load('{}', 'S');".format(mat))

        # sDFF - filtered traces (filtered how?)
        # F0 - background fluorescence (dims?)
        #print(' done')
        try:
            S = evil.eval('data.S')
        except matlab.engine.MatlabExecutionError:
            print('CNMF still needs to be run on this data')
            continue

        # TODO rename to indicate this is not just a filtered version of C?
        # (and how exactly is it different again?)
        # Loading w/ h5py lead to transposed indices wrt loading w/ MATLAB
        # engine.
        #filtered_df_over_f = np.array(S['sDFF']).T
        # TODO possible to just load a subfield of the CNM object / S w/ load
        # semantics?
        df_over_f = np.array(S['DFF']).T

        # TODO quantitatively compare sDFF / DFF. (s=smoothed)
        # maybe just upload non-smoothed and let people smooth downstream if
        # they want?

        # TODO could check for sCNM, to load that in cases where we can't make a
        # cnmf object? (but we should able able to here...)
        try:
            evil.evalc("clear; data = load('{}', 'CNM', 'ti');".format(mat))
        except matlab.engine.MatlabExecutionError as e:
            # TODO inspect error somehow to see if it's a memory error?
            # -> continue if so
            # TODO print to stderr
            print(e)
            continue

        raw_f = np.array(evil.eval('data.CNM.C')).T

        ti = evil.eval('data.ti')
        # TODO dtype appropriate?
        frame_times = np.array(ti['frame_times']).flatten()

        # Frame indices for CNMF output.
        # Of length equal to number of blocks. Each element is the frame
        # index (from 1) in CNMF output that starts the block, where
        # block is defined as a period of continuous acquisition.
        block_first_frames = np.array(ti['trial_start'], dtype=np.uint32
            ).flatten() - 1

        # stim_on is a number as above, but for the frame of the odor
        # onset.
        # TODO how does rounding work here? closest frame? first after?
        odor_onset_frames = np.array(ti['stim_on'], dtype=np.uint32
            ).flatten() - 1
        odor_offset_frames = np.array(ti['stim_off'], dtype=np.uint32
            ).flatten() - 1

        # TODO how to get odor pid

        # A has footprints
        # from caiman docs:
        # "A: ... (d x K)"
        # "K: ... # of neurons to extract"
        # so what is d? (looks like 256 x 256, the # of pixels)

        # TODO TODO maybe get some representation of the sparse matrix that i
        # can use to create one from the scipy constructors, to minimize amount
        # of data sent from matlab
        print('loading footprints...', end='')
        footprints = np.array(evil.eval('full(data.CNM.A)'))
        # Assuming equal number along both dimensions.
        pixels_per_side = int(np.sqrt(footprints.shape[0]))
        n_footprints = footprints.shape[1]

        # TODO C order? (check against image to make sure things don't seem
        # transposed...)
        footprints = np.reshape(footprints,
            (pixels_per_side, pixels_per_side, n_footprints))
        print(' done')

        # Just to try to free up some memory.
        evil.evalc('clear;')

        # TODO TODO TODO make sure these cell IDs match up with the ones from
        # below!!!

        footprint_dfs = []
        for cell_num in range(n_footprints):
            sparse = coo_matrix(footprints[:,:,cell_num])
            footprint_dfs.append(pd.DataFrame({
                'recording_from': [started_at],
                'cell': [cell_num],
                # Can be converted from lists of Python types, but apparently
                # not from numpy arrays or lists of numpy scalar types.
                # TODO check this doesn't transpose things
                # TODO TODO just move appropriate casting to my to_sql function,
                # and allow having numpy arrays (get type info from combination
                # of that and the database, like in other cases)
                'x_coords': [[int(x) for x in sparse.col.astype('int16')]],
                'y_coords': [[int(x) for x in sparse.row.astype('int16')]],
                'weights': [[float(x) for x in sparse.data.astype('float32')]],
                'analysis': [analysis_run]
            }))

        footprint_df = pd.concat(footprint_dfs, ignore_index=True)
        # TODO filter out footprints less than a certain # of pixels in cnmf?
        # (is 3 pixels really reasonable?)
        to_sql_with_duplicates(footprint_df, 'cells', verbose=True)

        # TODO and what would be a good db representation of footprint?
        # TODO TODO generalize to_sql casting to array types too?
        

        # TODO store image w/ footprint overlayed?
        # TODO TODO maybe store an average frame of registered TIF, and then
        # indexes around that per footprint? (explicitly try to avoid responses
        # when doing so, for easier interpretation as a background?)

        # dims=dimensions of image (256x256)
        # T is # of timestamps

        # TODO why 474 x 4 + 548 in one case? i thought frame numbers were
        # supposed to be more similar... (w/ np.diff(odor_onset_frames))
        first_onset_frame_offset = odor_onset_frames[0] - block_first_frames[0]

        n_frames, n_cells = df_over_f.shape
        assert n_cells == n_footprints

        start_frames = np.append(0,
            odor_onset_frames[1:] - first_onset_frame_offset)
        stop_frames = np.append(
            odor_onset_frames[1:] - first_onset_frame_offset - 1, n_frames)
        lens = [stop - start for start, stop in zip(start_frames, stop_frames)]

        # TODO delete version w/ int cast after checking they give same answers
        assert int(frame_times.shape[0]) == int(n_frames)
        assert frame_times.shape[0] == n_frames

        print(start_frames)
        print(stop_frames)
        # TODO find where the discrepancies are!
        print(sum(lens))
        print(n_frames)

        # TODO assert here that all frames add up / approx

        # TODO TODO either warn or err if len(start_frames) is !=
        # len(odor_pair_list)

        odor_id_pairs = [(o1,o2) for o1,o2 in zip(odor1_ids, odor2_ids)]

        comparison_num = -1

        for i in range(len(start_frames)):
            if i % (presentations_per_repeat * n_repeats) == 0:
                comparison_num += 1
                repeat_nums = {id_pair: 0 for id_pair in odor_id_pairs}

            # TODO TODO also save to csv/flat binary/hdf5 per (date, fly,
            # thorimage)
            print('Processing presentation {}'.format(i))

            start_frame = start_frames[i]
            stop_frame = stop_frames[i]
            # TODO off by one?? check
            # TODO check against frames calculated directly from odor offset...
            # may not be const # frames between these "starts" and odor onset?
            onset_frame = start_frame + first_onset_frame_offset

            # TODO check again that these are always equal and delete
            # "direct_onset_frame" bit
            print('onset_frame:', onset_frame)
            direct_onset_frame = odor_onset_frames[i]
            print('direct_onset_frame:', direct_onset_frame)

            # TODO TODO why was i not using direct_onset_frame for this before?
            onset_time = frame_times[direct_onset_frame]
            # TODO check these don't jump around b/c discontinuities
            presentation_frametimes = \
                frame_times[start_frame:stop_frame] - onset_time

            odor_pair = odor_id_pairs[i]
            odor1, odor2 = odor_pair
            repeat_num = repeat_nums[odor_pair]
            repeat_nums[odor_pair] = repeat_num + 1

            offset_frame = odor_offset_frames[i]
            print('offset_frame:', offset_frame)
            assert offset_frame > direct_onset_frame
            # TODO share more of this w/ dataframe creation below, unless that
            # table is changed to just reference presentation table
            presentation = pd.DataFrame({
                'prep_date': [date],
                'fly_num': fly_num,
                'recording_from': started_at,
                'comparison': comparison_num,
                'odor1': odor1,
                'odor2': odor2,
                'repeat_num': repeat_num,
                'odor_onset_frame': direct_onset_frame,
                'odor_offset_frame': offset_frame,
                # TODO TODO handle type conversion of array as necessary
                # + check it works
                'from_onset': [[float(x) for x in presentation_frametimes]],
                'analysis': analysis_run
            })
            to_sql_with_duplicates(presentation, 'presentations')


            # maybe share w/ code that checks distinct to decide whether to
            # load / analyze?
            key_cols = [
                'prep_date',
                'fly_num',
                'recording_from',
                'comparison',
                'odor1',
                'odor2',
                'repeat_num'
            ]
            db_presentations = pd.read_sql('presentations', conn,
                columns=(key_cols + ['presentation_id']))

            presentation_ids = (db_presentations[key_cols] ==
                                presentation[key_cols].iloc[0]).all(axis=1)
            assert presentation_ids.sum() == 1
            presentation_id = db_presentations.loc[presentation_ids,
                'presentation_id'].iat[0]

            # TODO get remy to save it w/ less than 64 bits of precision?
            presentation_dff = df_over_f[start_frame:stop_frame, :]
            presentation_raw_f = raw_f[start_frame:stop_frame, :]

            # Assumes that cells are indexed same here as in footprints.
            cell_dfs = []
            for cell_num in range(n_cells):

                cell_dff = presentation_dff[:, cell_num].astype('float32')
                cell_raw_f = presentation_raw_f[:, cell_num].astype('float32')

                cell_dfs.append(pd.DataFrame({
                    'presentation_id': [presentation_id],
                    'cell': [cell_num],
                    'df_over_f': [[float(x) for x in cell_dff]],
                    'raw_f': [[float(x) for x in cell_raw_f]]
                }))
            response_df = pd.concat(cell_dfs, ignore_index=True)

            to_sql_with_duplicates(response_df, 'responses')

            print('Done processing presentation {}'.format(i))

        # TODO check that all frames go somewhere and that frames aren't
        # given to two presentations. check they stay w/in block boundaries.
        # (they don't right now. fix!)

# TODO print unused stimfiles / option to delete them

