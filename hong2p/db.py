"""
Functions for interacting with (largely no longer used) (postgres) database
storing some experiment / analysis metadata.
"""

import os
from os.path import join, exists, getmtime
import sys
import socket
from datetime import datetime
import time

from hong2p import util, thor

# TODO test/check that all necessary imports were brought over in refactoring
# (+ all refs to util.py fns below are prefixed w/ 'util.')


db_hostname = os.getenv('HONG_POSTGRES_HOST', 'atlas')

# TODO TODO probably just move all stuff that uses db conn into it's own module
# under this package, and then just get the global conn upon that module import
conn = None
def get_db_conn():
    global conn
    global meta
    if conn is not None:
        return conn
    else:
        from sqlalchemy import create_engine, MetaData

        our_hostname = socket.gethostname()
        if our_hostname == db_hostname:
            url = 'postgresql+psycopg2://tracedb:tracedb@localhost:5432/tracedb'
        else:
            url = ('postgresql+psycopg2://tracedb:tracedb@{}' +
                ':5432/tracedb').format(db_hostname)

        conn = create_engine(url)

        # TODO this necessary? was it just for to_sql_with_duplicates or
        # something? why else?
        meta = MetaData()
        meta.reflect(bind=conn)

        return conn


# was too much trouble upgrading my python 3.6 caiman conda env to 3.7
'''
# This is a Python >=3.7 feature only.
def __getattr__(name):
    if name == 'conn':
        return get_db_conn()
    else:
        raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
'''

# TODO TODO can to_sql with pg_upsert replace this? what extra features did this
# provide?
def to_sql_with_duplicates(new_df, table_name, index=False, verbose=False):
    from sqlalchemy import MetaData, Table

    # TODO TODO document what index means / delete

    # TODO TODO if this fails and time won't be saved on reinsertion, any rows
    # that have been inserted already should be deleted to avoid confusion
    # (mainly, for the case where the program is interrupted while this is
    # running)
    # TODO TODO maybe have some cleaning step that checks everything in database
    # has the correct number of rows? and maybe prompts to delete?

    global conn
    if conn is None:
        conn = get_db_conn()

    # Other columns should be generated by database anyway.
    cols = list(new_df.columns)
    if index:
        cols += list(new_df.index.names)
    table_cols = ', '.join(cols)

    md = MetaData()
    table = Table(table_name, md, autoload_with=conn)
    dtypes = {c.name: c.type for c in table.c}

    if verbose:
        print('SQL column types:')
        pprint(dtypes)
   
    df_types = new_df.dtypes.to_dict()
    if index:
        df_types.update({n: new_df.index.get_level_values(n).dtype
            for n in new_df.index.names})

    if verbose:
        print('\nOld dataframe column types:')
        pprint(df_types)

    sqlalchemy2pd_type = {
        'INTEGER()': np.dtype('int32'),
        'SMALLINT()': np.dtype('int16'),
        'REAL()': np.dtype('float32'),
        'DOUBLE_PRECISION(precision=53)': np.dtype('float64'),
        'DATE()': np.dtype('<M8[ns]')
    }
    if verbose:
        print('\nSQL types to cast:')
        pprint(sqlalchemy2pd_type)

    new_df_types = {n: sqlalchemy2pd_type[repr(t)] for n, t in dtypes.items()
        if repr(t) in sqlalchemy2pd_type}

    if verbose:
        print('\nNew dataframe column types:')
        pprint(new_df_types)

    # TODO how to get around converting things to int if they have NaN.
    # possible to not convert?
    new_column_types = dict()
    new_index_types = dict()
    for k, t in new_df_types.items():
        if k in new_df.columns and not new_df[k].isnull().any():
            new_column_types[k] = t

        # TODO or is it always true that index level can't be NaN anyway?
        elif (k in new_df.index.names and
            not new_df.index.get_level_values(k).isnull().any()):

            new_index_types[k] = t

        # TODO print types being skipped b/c nan?

    new_df = new_df.astype(new_column_types, copy=False)
    if index:
        # TODO need to handle case where conversion dict is empty
        # (seems to fail?)
        #pprint(new_index_types)

        # MultiIndex astype method seems to not work the same way?
        new_df.index = pd.MultiIndex.from_frame(
            new_df.index.to_frame().astype(new_index_types, copy=False))

    # TODO print the type of any sql types not convertible?
    # TODO assert all dtypes can be converted w/ this dict?

    if index:
        print('writing to temporary table temp_{}...'.format(table_name))

    # TODO figure out how to profile
    new_df.to_sql('temp_' + table_name, conn, if_exists='replace', index=index,
        dtype=dtypes)

    # TODO change to just get column names?
    query = '''
    SELECT a.attname, format_type(a.atttypid, a.atttypmod) AS data_type
    FROM   pg_index i
    JOIN   pg_attribute a ON a.attrelid = i.indrelid
        AND a.attnum = ANY(i.indkey)
    WHERE  i.indrelid = '{}'::regclass
    AND    i.indisprimary;
    '''.format(table_name)
    result = conn.execute(query)
    pk_cols = ', '.join([n for n, _ in result])

    # TODO TODO TODO modify so on conflict the new row replaces the old one!
    # (for updates to analysis, if exact code version w/ uncommited changes and
    # everything is not going to be part of primary key...)
    # (want updates to non-PK rows)

    # TODO TODO should i just delete rows w/ our combination(s) of pk_cols?
    # (rather than other upsert strategies)
    # TODO (flag to) check deletion was successful
    # TODO factor deletion into another fn (?) and expose separately in gui


    # TODO prefix w/ ANALYZE EXAMINE and look at results
    query = ('INSERT INTO {0} ({1}) SELECT {1} FROM temp_{0} ' +
        'ON CONFLICT ({2}) DO NOTHING').format(table_name, table_cols, pk_cols)
    # TODO maybe a merge is better for this kind of upsert, in postgres?
    if index:
        # TODO need to stdout flush or something?
        print('inserting into {} from temporary table... '.format(table_name),
            end='')

    # TODO let this happen async in the background? (don't need result)
    conn.execute(query)

    # TODO flag to read back and check insertion stored correct data?

    if index:
        print('done')

    # TODO drop staging table


def pg_upsert(table, conn, keys, data_iter):
    from sqlalchemy.dialects import postgresql
    # https://github.com/pandas-dev/pandas/issues/14553
    for row in data_iter:
        row_dict = dict(zip(keys, row))
        sqlalchemy_table = meta.tables[table.name]
        stmt = postgresql.insert(sqlalchemy_table).values(**row_dict)
        upsert_stmt = stmt.on_conflict_do_update(
            index_elements=table.index,
            set_=row_dict)
        conn.execute(upsert_stmt)


def upload_code_info(code_versions):
    """Returns a list of integer IDs for inserted code version rows.

    code_versions should be a list of dicts, each dict representing a row in the
    corresponding table.
    """
    global conn
    if conn is None:
        conn = get_db_conn()

    if len(code_versions) == 0:
        raise ValueError('code versions can not be empty')

    code_versions_df = pd.DataFrame(code_versions)
    # TODO delete try/except
    try:
        code_versions_df.to_sql('code_versions', conn, if_exists='append',
            index=False)
    except:
        print(code_versions_df)
        import ipdb; ipdb.set_trace()

    # TODO maybe only read most recent few / restrict to some other key if i
    # make one?
    db_code_versions = pd.read_sql('code_versions', conn)

    our_version_cols = code_versions_df.columns
    version_ids = list()
    for _, row in code_versions_df.iterrows():
        # This should take the *first* row that is equal.
        idx = (db_code_versions[code_versions_df.columns] == row).all(
            axis=1).idxmax()
        version_id = db_code_versions['version_id'].iat[idx]
        assert version_id not in version_ids
        version_ids.append(version_id)

    return version_ids


def upload_analysis_info(*args) -> None:
    """
    Requires that corresponding row in analysis_runs table already exists,
    if only two args are passed.
    """
    global conn
    if conn is None:
        conn = get_db_conn()

    have_ids = False
    if len(args) == 2:
        analysis_started_at, code_versions = args
    elif len(args) == 3:
        recording_started_at, analysis_started_at, code_versions = args

        if len(code_versions) == 0:
            raise ValueError('code_versions can not be empty')

        if type(code_versions) == list and np.issubdtype(
            type(code_versions[0]), np.integer):

            version_ids = code_versions
            have_ids = True

        pd.DataFrame({
            'run_at': [analysis_started_at],
            'recording_from': recording_started_at
        }).set_index('run_at').to_sql('analysis_runs', conn,
            if_exists='append', method=pg_upsert)

    else:
        raise ValueError('incorrect number of arguments')

    if not have_ids:
        version_ids = upload_code_info(code_versions)

    analysis_code = pd.DataFrame({
        'run_at': analysis_started_at,
        'version_id': version_ids
    })
    to_sql_with_duplicates(analysis_code, 'analysis_code')


def list_segmentations(tif_path):
    """Returns a DataFrame of segmentation_runs for given motion corrected TIFF.
    """
    global conn
    if conn is None:
        conn = get_db_conn()

    # TODO could maybe turn these two queries into one (WITH semantics?)
    # TODO TODO should maybe trim all prefixes from input_filename before
    # uploading? unless i want to just figure out path from other variables each
    # time and use that to match (if NAS_PREFIX is diff, there will be no match)
    prefix = analysis_output_root()
    if tif_path.startswith(prefix):
        tif_path = tif_path[len(prefix):]

    # TODO test this strpos stuff is equivalent to where input_filename = x
    # in case where prefixes are the same
    analysis_runs = pd.read_sql_query('SELECT * FROM analysis_runs WHERE ' +
        "strpos(input_filename, '{}') > 0".format(tif_path), conn)

    if len(analysis_runs) == 0:
        return None

    # TODO better way than looping over each of these? move to sql query?
    analysis_start_times = analysis_runs.run_at.unique()
    seg_runs = []
    for run_at in analysis_start_times:
        seg_runs.append(pd.read_sql_query('SELECT * FROM segmentation_runs ' +
            "WHERE run_at = '{}'".format(pd.Timestamp(run_at)), conn
        ))

        # TODO maybe merge w/ analysis_code (would have to yield multiple rows
        # per segmentation run when multiple code versions referenced)

    seg_runs = pd.concat(seg_runs, ignore_index=True)
    if len(seg_runs) == 0:
        return None

    seg_runs = seg_runs.merge(analysis_runs)
    seg_runs.sort_values('run_at', inplace=True)
    return seg_runs


def merge_odors(df, *args):
    global conn
    if conn is None:
        conn = get_db_conn()

    if len(args) == 0:
        odors = pd.read_sql('odors', conn)
    elif len(args) == 1:
        odors = args[0]
    else:
        raise ValueError('incorrect number of arguments')

    print('merging with odors table...', end='', flush=True)
    # TODO way to do w/o resetting index? merge failing to find odor1 or just
    # drop?
    # TODO TODO TODO do i want drop=True? (it means cols in index won't be
    # inserted into dataframe...) check use of merge_odors and change to
    # drop=False (default) if it won't break anything
    df = df.reset_index(drop=True)

    df = pd.merge(df, odors, left_on='odor1', right_on='odor_id',
                  suffixes=(False, False))

    df.drop(columns=['odor_id','odor1'], inplace=True)
    df.rename(columns={'name': 'name1',
        'log10_conc_vv': 'log10_conc_vv1'}, inplace=True)

    df = pd.merge(df, odors, left_on='odor2', right_on='odor_id',
                  suffixes=(False, False))

    df.drop(columns=['odor_id','odor2'], inplace=True)
    df.rename(columns={'name': 'name2',
        'log10_conc_vv': 'log10_conc_vv2'}, inplace=True)

    print(' done')

    # TODO refactor merge fns to share some stuff? (progress, length checking,
    # arg unpacking, etc)?
    return df


def merge_recordings(df, *args, verbose=True):
    global conn
    if conn is None:
        conn = get_db_conn()

    if len(args) == 0:
        recordings = pd.read_sql('recordings', conn)
    elif len(args) == 1:
        recordings = args[0]
    else:
        raise ValueError('incorrect number of arguments')

    print('merging with recordings table...', end='', flush=True)
    len_before = len(df)
    # TODO TODO TODO do i want drop=True? (it means cols in index won't be
    # inserted into dataframe...) check use of this fn and change to
    # drop=False (default) if it won't break anything
    df = df.reset_index(drop=True)

    df = pd.merge(df, recordings, how='left', left_on='recording_from',
        right_on='started_at', suffixes=(False, False))

    df.drop(columns=['started_at'], inplace=True)

    # TODO TODO see notes in kc_analysis about sub-recordings and how that
    # will now break this in the recordings table
    # (multiple dirs -> one start time)
    df['thorimage_id'] = df.thorimage_path.apply(lambda x: split(x)[-1])
    assert len_before == len(df), 'merging changed input df length'
    print(' done')
    return df


def merge_gsheet(df, *args, use_cache=False):
    """
    df must have a column named either 'recording_from' or 'started_at'

    gsheet rows get this information by finding the right ThorImage
    Experiment.xml files on the NAS and loading them for this timestamp.
    """
    if len(args) == 0:
        gsdf = mb_team_gsheet(use_cache=use_cache)
    elif len(args) == 1:
        # TODO maybe copy in this case?
        gsdf = args[0]
    else:
        raise ValueError('incorrect number of arguments')

    if 'recording_from' in df.columns:
        # TODO maybe just merge_recordings w/ df in advance in this case?
        df = df.rename(columns={'recording_from': 'started_at'})
    elif 'started_at' not in df.columns:
        raise ValueError("df needs 'recording_from'/'started_at' in columns")

    gsdf['recording_from'] = pd.NaT
    for i, row in gsdf.iterrows():
        date_dir = row.date.strftime(date_fmt_str)
        fly_num = str(int(row.fly_num))
        image_dir = join(raw_data_root(),
            date_dir, fly_num, row.thorimage_dir
        )
        thorimage_xml_path = join(image_dir, 'Experiment.xml')

        try:
            xml_root = thor.xmlroot(thorimage_xml_path)
        except FileNotFoundError as e:
            continue

        gsdf.loc[i, 'recording_from'] = thor.get_thorimage_time_xml(xml_root)

    # TODO fail if stuff marked attempt_analysis has missing xml files?
    # or if nothing was found?

    gsdf = gsdf.rename(columns={'date': 'prep_date'})

    return merge_recordings(gsdf, df, verbose=False)


def accepted_blocks(analysis_run_at, verbose=False):
    """
    """
    global conn
    if conn is None:
        conn = get_db_conn()

    if verbose:
        print('entering accepted_blocks')

    analysis_run_at = pd.Timestamp(analysis_run_at)
    presentations = pd.read_sql_query('SELECT presentation_id, ' +
        'comparison, presentation_accepted FROM presentations WHERE ' +
        "analysis = '{}'".format(analysis_run_at), conn,
        index_col='comparison')
    # TODO any of stuff below behave differently if index is comparison
    # (vs. default range index)? groupby('comparison')?

    analysis_run = pd.read_sql_query('SELECT accepted, input_filename, ' +
        "recording_from FROM analysis_runs WHERE run_at = '{}'".format(
        analysis_run_at), conn)
    assert len(analysis_run) == 1
    analysis_run = analysis_run.iloc[0]
    recording_from = analysis_run.recording_from
    input_filename = analysis_run.input_filename
    all_blocks_accepted = analysis_run.accepted

    # TODO TODO make sure block bounds are loaded into db from gui first, if
    # they changed in the gsheet. otherwise, will be stuck using old values, and
    # this function will not behave correctly
    # TODO TODO this has exactly the same problem canonical_segmentation
    # currently has: only one of each *_block per recording start time =>
    # sub-recordings will clobber each other. fix!
    # (currently just working around w/ subrecording tif filename hack)
    recording = pd.read_sql_query('SELECT thorimage_path, first_block, ' +
        "last_block FROM recordings WHERE started_at = '{}'".format(
        recording_from), conn)

    assert len(recording) == 1
    recording = recording.iloc[0]

    # TODO delete this if not going to use it to calculate uploaded_block_info
    if len(presentations) > 0:
        presentations_with_responses = pd.read_sql_query('SELECT ' +
            'presentation_id FROM responses WHERE segmentation_run = ' +
            "'{}'".format(analysis_run_at), conn)
        # TODO faster to check isin if this is a set?
    #

    # TODO TODO implement some kind of handling of sub-recordings in db
    # and get rid of this hack
    #print(input_filename)
    if is_subrecording_tiff(input_filename):
        first_block, last_block = subrecording_tiff_blocks(input_filename)

        if verbose:
            print(input_filename, 'belonged to a sub-recording')

    else:
        if recording.last_block is None or recording.first_block is None:
            # TODO maybe generate it in this case?
            raise ValueError(('no block info in db for recording_from = {} ({})'
                ).format(recording_from, recording.thorimage_path))

        first_block = recording.first_block
        last_block = recording.last_block

    n_blocks = last_block - first_block + 1
    expected_comparisons = list(range(n_blocks))

    # TODO delete these prints. for debugging.
    if verbose:
        print('presentations:', presentations)
        print('expected_comparisons:', expected_comparisons)
        print('all_blocks_accepted:', all_blocks_accepted)
    #
    # TODO TODO TODO check that upload will keep comparison numbered as blocks
    # are, so that missing comparisons numbers can be imputed with False here
    # (well, comparison numbering should probably start w/ 0 at first_block)

    # TODO TODO test cases where not all blocks were uploaded at all, where some
    # where not uploaded and some are explicitly marked not accepted, and where
    # all blocks rejected are explicitly so

    if pd.notnull(all_blocks_accepted):
        fill_value = all_blocks_accepted
    else:
        fill_value = False

    def block_accepted(presentation_df):
        null = pd.isnull(presentation_df.presentation_accepted)
        if null.any():
            assert null.all()
            return fill_value

        accepted = presentation_df.presentation_accepted
        if accepted.any():
            assert accepted.all()
            return True
        else:
            return False

    '''
    null_presentation_accepted = \
        pd.isnull(presentations.presentation_accepted)
    if null_presentation_accepted.any():
        if verbose:
            print('at least one presentation was null')
        # TODO fix db w/ a script or just always check for old way of doing it?
        # fixing db would mean translating all analysis_runs.accepted into
        # presentations.presentation_accepted and then deleting
        # analysis_runs.accepted column

        assert null_presentation_accepted.all(), 'not all null'
        assert not pd.isnull(all_blocks_accepted),'all_blocks_accepted null'

        if all_blocks_accepted:
            accepted = [True] * n_blocks
        else:
            accepted = [False] * n_blocks
    else:
        if verbose:
            print('no presentations were null')
    '''
    # TODO make sure sorted by comparison #. groupby ensure that?
    accepted = presentations.groupby('comparison'
        ).agg(block_accepted).presentation_accepted
    accepted.name = 'comparison_accepted'
    assert len(accepted.shape) == 1, 'accepted was not a Series'

    if verbose:
        print('accepted before filling missing values:', accepted)

    if (((accepted == True).any() and all_blocks_accepted == False) or
        ((accepted == False).any() and all_blocks_accepted)):
        # TODO maybe just correct db in this case?
        # (set analysis_run.accepted to null and keep presentation_accepted
        # if inconsistent / fill them from analysis_run.accepted if missing)
        #raise ValueError('inconsistent accept labels')
        warnings.warn('inconsistent accept labels. '
            'nulling analysis_runs.accepted in corresponding row.'
        )

        # TODO TODO test this!
        sql = ('UPDATE presentations SET presentation_accepted = {} WHERE ' +
            "analysis = '{}' AND presentation_accepted IS NULL").format(
            fill_value, analysis_run_at)
        ret = conn.execute(sql)
        # TODO if i'm gonna call this multiple times, maybe just factor it into
        # a fn
        presentations_after_update = pd.read_sql_query(
            'SELECT presentation_id, ' +
            'comparison, presentation_accepted FROM presentations WHERE ' +
            "analysis = '{}'".format(analysis_run_at), conn,
            index_col='comparison')
        if verbose:
            print('Presentations after filling w/ all_blocks_accepted:')
            print(presentations_after_update)

        sql = ('UPDATE analysis_runs SET accepted = NULL WHERE run_at = ' +
            "'{}'").format(analysis_run_at)
        ret = conn.execute(sql)

    # TODO TODO TODO are this case + all_blocks_accepted=False case in if
    # above the only two instances where the block info is not uploaded (or
    # should be, assuming no accept of non-uploaded experiment)
    for c in expected_comparisons:
        if c not in accepted.index:
            accepted.loc[c] = fill_value

    accepted = accepted.to_list()

    # TODO TODO TODO also calculate and return uploaded_block_info
    # based on whether a given block has (all) of it's presentations and
    # responses entries (whether accepted or not)
    if verbose:
        print('leaving accepted_blocks\n')
    return accepted


def print_all_accepted_blocks():
    """Just for testing behavior of accepted_blocks fn.
    """
    global conn
    if conn is None:
        conn = get_db_conn()

    analysis_runs = pd.read_sql_query('SELECT run_at FROM segmentation_runs',
        conn).run_at

    for r in analysis_runs:
        try:
            print('{}: {}'.format(r, accepted_blocks(r)))
        except ValueError as e:
            print(e)
            continue
        #import ipdb; ipdb.set_trace()

    import ipdb; ipdb.set_trace()


# TODO maybe delete this. doesn't seem used anywhere... (honestly maybe all of
# the db code...)
def latest_analysis(verbose=False):
    global conn
    if conn is None:
        conn = get_db_conn()

    # TODO sql based command to get analysis info for stuff that has its
    # timestamp in segmentation_runs, to condense these calls to one?
    seg_runs = pd.read_sql_query('SELECT run_at FROM segmentation_runs',
        conn)
    analysis_runs = pd.read_sql('analysis_runs', conn)
    seg_runs = seg_runs.merge(analysis_runs)

    seg_runs.input_filename = seg_runs.input_filename.apply(lambda t:
        t.split('mb_team/analysis_output/')[1])

    # TODO TODO change all path handling to be relative to NAS_PREFIX.
    # don't store absolute paths (or if i must, also store what prefix is?)
    input_tifs = seg_runs.input_filename.unique()
    has_subrecordings = set()
    key2tiff = dict()
    # TODO decide whether i want this to be parent_id or thorimage_id
    # maybe make a kwarg flag to this fn to switch between them
    tiff2parent_id = dict()
    tiff_is_subrecording = dict()
    for tif in input_tifs:
        if verbose:
            print(tif, end='')

        date_fly_keypart = '/'.join(tif.split('/')[:2])
        thorimage_id = tiff_thorimage_id(tif)
        key = '{}/{}'.format(date_fly_keypart, thorimage_id)
        # Assuming this is 1:1 for now.
        key2tiff[key] = tif
        try:
            parent_id = parent_recording_id(tif)
            tiff_is_subrecording[tif] = True
            parent_key = '{}/{}'.format(date_fly_keypart, parent_id)
            has_subrecordings.add(parent_key)
            tiff2parent_id[tif] = parent_id

            if verbose:
                print(' (sub-recording of {})'.format(parent_key))

        # This is triggered if tif is not a sub-recording.
        except ValueError:
            tiff_is_subrecording[tif] = False
            tiff2parent_id[tif] = thorimage_id
            if verbose:
                print('')

    nonoverlapping_input_tifs = set(t for k, t in key2tiff.items()
                                 if k not in has_subrecordings)
    # set(input_tifs) - nonoverlapping_input_tifs

    # TODO if verbose, maybe also print stuff tossed for having subrecordings
    # as well as # rows tossed for not being accepted / stuff w/o analysis /
    # stuff w/o anything accepted

    # TODO between this and the above, make sure to handle (ignore) stuff that
    # doesn't have any analysis done.
    seg_runs = seg_runs[seg_runs.input_filename.isin(nonoverlapping_input_tifs)]

    # TODO TODO and if there are disjoint sets of accepted blocks, would ideally
    # return something indicating which analysis to get which block from?  would
    # effectively have to search per block/comparison, right?
    # TODO would ideally find the latest analysis that has the maximal
    # number of blocks accepted (per input file) (assuming just going to return
    # one analysis version per tif, rather than potentially a different one for
    # each block)
    seg_runs['n_accepted_blocks'] = seg_runs.run_at.apply(lambda r:
        sum(accepted_blocks(r)))
    accepted_runs = seg_runs[seg_runs.n_accepted_blocks > 0]

    latest_tif_analyses = accepted_runs.groupby('input_filename'
        ).run_at.max().to_frame()
    latest_tif_analyses['is_subrecording'] = \
        latest_tif_analyses.index.map(tiff_is_subrecording)

    subrec_blocks = latest_tif_analyses.apply(subrecording_tiff_blocks_df,
        axis=1, result_type='expand')
    latest_tif_analyses[['first_block','last_block']] = subrec_blocks

    latest_tif_analyses['thorimage_id'] = \
        latest_tif_analyses.index.map(tiff2parent_id)

    # TODO what format would make the most sense for the output?
    # which index? just trimmed input_filename? recording_from (+ block /
    # comparison)? (fly, date, [thorimage_id? recording_from?] (+ block...)
    # ?
    return latest_tif_analyses


def sql_timestamp_list(df):
    """
    df must have a column run_at, that is a pandas Timestamp type
    """
    timestamp_list = '({})'.format(', '.join(
        ["'{}'".format(x) for x in df.run_at]
    ))
    return timestamp_list


# TODO w/ this or a separate fn using this, print what we have formatted
# roughly like in data_tree in gui, so that i can check it against the gui
# TODO maybe delete. seems unused.
def latest_analysis_presentations(analysis_run_df):
    global conn
    if conn is None:
        conn = get_db_conn()

    # TODO maybe compare time of this to getting all and filtering locally
    # TODO at least once, compare the results of this to filtering locally
    # IS NOT DISTINCT FROM should also 
    presentations = pd.read_sql_query('SELECT * FROM presentations WHERE ' +
        '(presentation_accepted = TRUE OR presentation_accepted IS NULL) ' +
        'AND analysis IN ' + sql_timestamp_list(analysis_run_df), conn)

    # TODO TODO maybe just do a migration on the db to fix all comparisons
    # to not have to be renumbered, and fix gui(+kc_natural_mixes/populate_db?)
    # so they don't restart numbering across sub-recordings that come from same
    # recording?

    # TODO TODO TODO test that this is behaving as expected
    # - is there only one place where presentatinos.analysis == row.run_at?
    #   assert that?
    # - might the sample things ever get incremented twice?
    for row in analysis_run_df[analysis_run_df.is_subrecording].itertuples():
        run_presentations = (presentations.analysis == row.run_at)
        presentations.loc[run_presentations, 'comparison'] = \
            presentations[run_presentations].comparison + int(row.first_block)

        # TODO check that these rows are also complete / valid

    # TODO use those check fns on these presentations, to make sure they are
    # full blocks and stuff

    # TODO ultimately, i want all of these latest_* functions to return a
    # dataframe without an analysis column (still return it, just in case it
    # becomes necessary later?)
    # (or at least i want to make sure that the other index columns can uniquely
    # refer to something, s.t. adding analysis to a drop_duplicates call does
    # not change the total # of returned de-duplicated rows)
    # TODO which index cols again?

    return presentations


# TODO maybe delete. seems unused.
def latest_analysis_footprints(analysis_run_df):
    global conn
    if conn is None:
        conn = get_db_conn()

    footprints = pd.read_sql_query(
        'SELECT * FROM cells WHERE segmentation_run IN ' +
        sql_timestamp_list(analysis_run_df), conn)
    return footprints


# TODO maybe delete. seems unused.
def latest_analysis_traces(df):
    """
    Input DataFrame must have a presentation_id column matching that in the db.
    This way, presentations already filtered to be the latest just get their
    responses assigned too them.
    """
    global conn
    if conn is None:
        conn = get_db_conn()

    responses = pd.read_sql_query(
        'SELECT * FROM responses WHERE presentation_id IN ' +
        '({})'.format(','.join([str(x) for x in df.presentation_id])), conn)
    # responses should by larger by a factor of # cells within each analysis run
    assert len(df) == len(responses.presentation_id.unique())
    return responses
    

response_stat_cols = [
    'exp_scale',
    'exp_tau',
    'exp_offset',
    'exp_scale_sigma',
    'exp_tau_sigma',
    'exp_offset_sigma',
    'avg_dff_5s',
    'avg_zchange_5s'
]
def latest_response_stats(*args):
    """
    """
    global conn
    if conn is None:
        conn = get_db_conn()

    index_cols = [
        'prep_date',
        'fly_num',
        'recording_from',
        'analysis',
        'comparison',
        'odor1',
        'odor2',
        'repeat_num'
    ]
    # TODO maybe just get cols db has and exclude from_onset or something?
    # just get all?
    presentation_cols_to_get = index_cols + response_stat_cols
    if len(args) == 0:
        db_presentations = pd.read_sql('presentations', conn,
            columns=(presentation_cols_to_get + ['presentation_id']))

    elif len(args) == 1:
        db_presentations = args[0]

    else:
        raise ValueError('too many arguments. expected 0 or 1')

    referenced_recordings = set(db_presentations['recording_from'].unique())

    if len(referenced_recordings) == 0:
        return

    db_analysis_runs = pd.read_sql('analysis_runs', conn,
        columns=['run_at', 'recording_from', 'accepted'])
    db_analysis_runs.set_index(['recording_from', 'run_at'],
        inplace=True)

    # Making sure not to get multiple presentation entries referencing the same
    # real presentation in any single recording.
    presentation_stats = []
    for r in referenced_recordings:
        # TODO are presentation->recording and presentation->
        # analysis_runs->recording inconsistent somehow?
        # TODO or is this an insertion order thing? rounding err?
        # maybe set_index is squashing stuff?
        # TODO maybe just stuff i'm skipping now somehow?

        # TODO TODO merge db_analysis_runs w/ recordings to get
        # thorimage_dir / id for troubleshooting?
        # TODO fix and delete try / except
        try:
            rec_analysis_runs = db_analysis_runs.loc[(r,)]
        except KeyError:
            # TODO should this maybe be an error?
            '''
            print(db_analysis_runs)
            print(referenced_recordings)
            print(r)
            import ipdb; ipdb.set_trace()
            '''
            warnings.warn('referenced recording not in analysis_runs!')
            continue

        # TODO TODO TODO switch to using presentations.presentation_accepted
        raise NotImplementedError
        rec_usable = rec_analysis_runs.accepted.any()

        rec_presentations = db_presentations[
            db_presentations.recording_from == r]

        # TODO maybe use other fns here to check it has all repeats / full
        # comparisons?

        for g, gdf in rec_presentations.groupby(
            ['comparison', 'odor1', 'odor2', 'repeat_num']):

            # TODO rename (maybe just check all response stats at this point...)
            # maybe just get most recent row that has *any* of them?
            # (otherwise would have to combine across rows...)
            has_exp_fit = gdf[gdf.exp_scale.notnull()]

            # TODO compute if no response stats?
            if len(has_exp_fit) == 0:
                continue

            most_recent_fit_idx = has_exp_fit.analysis.idxmax()
            most_recent_fit = has_exp_fit.loc[most_recent_fit_idx].copy()

            assert len(most_recent_fit.shape) == 1

            # TODO TODO TODO switch to using presentations.presentation_accepted
            raise NotImplementedError
            most_recent_fit['accepted'] = rec_usable

            # TODO TODO TODO clean up older fits on same data?
            # (delete from database)
            # (if no dependent objects...)
            # probably somewhere else...?

            presentation_stats.append(most_recent_fit.to_frame().T)

    if len(presentation_stats) == 0:
        return

    presentation_stats_df = pd.concat(presentation_stats, ignore_index=True)

    # TODO just convert all things that look like floats?

    for c in response_stat_cols:
        presentation_stats_df[c] = presentation_stats_df[c].astype('float64')

    for date_col in ('prep_date', 'recording_from'):
        presentation_stats_df[date_col] = \
            pd.to_datetime(presentation_stats_df[date_col])

    return presentation_stats_df


# TODO at least rename to something like
# 'load_[preprocessed/corrected]_recording' as long as it requires [mocorred,
# right?] tiff. maybe also something experiment specific as long as it requires
# spreadsheet i might discontinue use of in general / as long as stimulus
# handling is ~experiment specific.
# TODO or maybe just move this + other project specific stuff to their own repo
# that uses hong2p, similar to what i'm doing w/ al_pair_grids
def load_recording(tiff, allow_gsheet_to_restrict_blocks=True,
    allow_missing_odor_presentations=False, verbose=True):
    # TODO summarize the various errors this could possibly raise
    # probably at least valueerror, assertionerror, ioerror (?), & tifffile
    # memory error
    # TODO TODO try to add a flag to force/allow this fn to operate
    # independently of the postgres database
    """
    May raise errors if some part of the loading fails.
    """
    import tifffile
    import ijroi
    from scipy.sparse import coo_matrix

    import chemutils as cu

    # TODO test that all of this works w/ both raw & motion corrected tiffs
    # (named and placed according to convention)
    keys = tiff_filename2keys(tiff)
    recording_title = format_keys(*keys)
    if verbose:
        print(recording_title)

    start = time.time()

    mat = matfile(*keys)
    # For some of the older data, need to either modify scipy loadmat call or
    # revert to use_matlab_engine=True call.
    ti = matlab.load_mat_timing_info(mat)
    frame_times = ti['frame_times']
    block_first_frames = ti['block_first_frames']
    block_last_frames = ti['block_last_frames']
    odor_onset_frames = ti['odor_onset_frames']
    odor_offset_frames = ti['odor_offset_frames']
    del ti
    # Copying so we can subset while still checking the original values
    # against the freshly loaded movie.
    orig_frame_times = frame_times.copy()
    orig_block_first_frames = block_first_frames.copy()
    orig_block_last_frames = block_last_frames.copy()

    # TODO TODO also store (the contents of this) in db?
    # This will return defaults if the YAML file is not found.
    meta = metadata(*keys)
    drop_first_n_frames = meta['drop_first_n_frames']
    # TODO TODO err if this is past first odor onset (or probably even too
    # close)
    del meta

    df = mb_team_gsheet()
    recordings = df.loc[
        (df.date == keys.date) &
        (df.fly_num == keys.fly_num) &
        (df.thorimage_dir == keys.thorimage_id)
    ]
    del df
    recording = recordings.iloc[0]
    del recordings
    if recording.project != 'natural_odors':
        raise ValueError('project type {} not supported'.format(
            recording.project
        ))

    stimfile = recording['stimulus_data_file']
    first_block = recording['first_block']
    last_block = recording['last_block']
    sync_dir = thorsync_dir(*keys[:2], recording['thorsync_dir'])
    image_dir = thorimage_dir(*keys[:2], recording['thorimage_dir'])
    del recording

    data = load_odor_metadata(stimfile)
    odor_list = data['odor_list']
    n_repeats = data['n_repeats']
    presentations_per_repeat = data['presentations_per_repeat']
    presentations_per_block = data['presentations_per_block']
    pair_case = data['pair_case']

    # TODO TODO maybe `allow_gsheet_to_restrict` block should also influence
    # whether the values in `recording` are used? (unless i want to just
    # unsupport the False case...)
    if pd.isnull(first_block):
        first_block = 0
    else:
        first_block = int(first_block) - 1

    if pd.isnull(last_block):
        n_full_panel_blocks = \
            int(len(odor_list) / presentations_per_block)
        last_block = n_full_panel_blocks - 1
    else:
        last_block = int(last_block) - 1

    xml = thor.get_thorimage_xmlroot(image_dir)
    started_at = thor.get_thorimage_time_xml(xml)

    # TODO upload full_frame_avg_trace like in kc_natural_mixes/populate_db?
    recording_df = pd.DataFrame({
        'started_at': [started_at],
        'thorsync_path': [sync_dir],
        'thorimage_path': [image_dir],
        'stimulus_data_path': [stimfile],
        'first_block': [first_block],
        'last_block': [last_block],
        'n_repeats': [n_repeats],
        'presentations_per_repeat': [presentations_per_repeat]
        # TODO but do i want the trace of the full movie or slice?
        # if full movie, probably should just calculate once, then
        # slice that for the trace i use here
        #'full_frame_avg_trace': 
    })
    # TODO at least put behind ACTUALLY_UPLOAD?
    # TODO maybe defer this to accepting?
    # TODO TODO also replace other cases that might need to be
    # updated w/ pg_upsert based solution
    # TODO just completely delete this fn at this point?
    ##to_sql_with_duplicates(recording_df, 'recordings')
    recording_df.set_index('started_at').to_sql('recordings', conn,
        if_exists='append', method=pg_upsert
    )
    db_recording = pd.read_sql_query('SELECT * FROM recordings WHERE ' +
        "started_at = '{}'".format(pd.Timestamp(started_at)), conn
    )
    db_recording = db_recording[recording_df.columns]
    assert recording_df.equals(db_recording)
    del db_recording

    # TODO maybe use subset here too, to be consistent w/ which mixtures get
    # entered... (according to the blocks that are ultimately used, right?
    # something else?)
    if pair_case:
        odors = pd.DataFrame({
            'name': data['odors'],
            'log10_conc_vv': [0 if x == 'paraffin' else
                natural_odors_concentrations.at[x,
                'log10_vial_volume_fraction'] for x in data['odors']]
        })
    else:
        # TODO fix db to represent arbitrary mixtures more generally,
        # so this hack isn't necessary
        # TODO TODO fix entries already in db w/ trailing / leading
        # whitespace (split_odor_w_conc didn't used to strip returned name)
        # TODO TODO TODO fix what generates broken data['odors'] in at least
        # some of the fly food cases (missing fly food b)
        '''
        odors = pd.DataFrame([split_odor_w_conc(x) for x in
            (data['odors'] + ['no_second_odor'])
        ])
        '''
        odors = pd.DataFrame([split_odor_w_conc(x) for x in
            (list(set(data['odor_lists'])) + ['no_second_odor'])
        ])

    to_sql_with_duplicates(odors, 'odors')

    # TODO make unique id before insertion? some way that wouldn't require
    # the IDs, but would create similar tables?

    first_presentation = first_block * presentations_per_block
    last_presentation = (last_block + 1) * presentations_per_block - 1

    odor_list = odor_list[first_presentation:(last_presentation + 1)]
    assert (len(odor_list) % (presentations_per_repeat * n_repeats) == 0)

    # TODO set gui segmentation widget self.db_odors w/ this value at return /
    # recompute out there
    db_odors = pd.read_sql('odors', conn)
    db_odors.set_index(['name', 'log10_conc_vv'],
        verify_integrity=True, inplace=True
    )

    # TODO invert to check
    # TODO is this sql table worth anything if both keys actually need to be
    # referenced later anyway? (?)

    # TODO TODO TODO modify to work w/ any output of cutpaste generator
    # (arbitrary len lists in odor_lists) (see cutpaste code for similar
    # problem?)
    # TODO only add as many as there were blocks from thorsync timing info?
    if pair_case:
        # TODO this rounding to 5 decimal places always work?
        o2c = odors.set_index('name', verify_integrity=True
            ).log10_conc_vv.round(decimals=5)

        odor1_ids = [db_odors.at[(o1, o2c[o1]), 'odor_id']
            for o1, _ in odor_list
        ]
        odor2_ids = [db_odors.at[(o2, o2c[o2]), 'odor_id']
            for _, o2 in odor_list
        ]
        del o2c
    else:
        odor1_ids = [db_odors.at[tuple(split_odor_w_conc(o)),
            'odor_id'] for o in odor_list
        ]

        # TODO fix db to represent arbitrary mixtures more generally,
        # so this hack isn't necessary
        no_second_odor_id = db_odors.at[
            ('no_second_odor', 0.0), 'odor_id'
        ]
        odor2_ids = [no_second_odor_id] * len(odor1_ids)

    # TODO make unique first. only need order for filling in the
    # values in responses. (?)
    # TODO wait, how is this associated w/ anything else in this run?
    # is this table even used?
    mixtures = pd.DataFrame({
        'odor1': odor1_ids,
        'odor2': odor2_ids
    })
    # TODO maybe defer this to accepting...
    to_sql_with_duplicates(mixtures, 'mixtures')

    # TODO rename to indicate it's for each presentation / stimulus / trial?
    odor_ids = list(zip(odor1_ids, odor2_ids))

    n_blocks_from_gsheet = last_block - first_block + 1
    assert len(odor_list) == n_blocks_from_gsheet * presentations_per_block

    n_blocks_from_thorsync = len(block_first_frames)
    err_msg = ('{} blocks ({} to {}, inclusive) in Google sheet {{}} {} ' +
        'blocks from ThorSync.').format(n_blocks_from_gsheet,
        first_block + 1, last_block + 1, n_blocks_from_thorsync
    )
    fail_msg = (' Fix in Google sheet, turn off '
        'cache if necessary, and rerun.'
    )

    if n_blocks_from_gsheet > n_blocks_from_thorsync:
        raise ValueError(err_msg.format('>') + fail_msg)

    elif n_blocks_from_gsheet < n_blocks_from_thorsync:
        if allow_gsheet_to_restrict_blocks:
            warnings.warn(err_msg.format('<') + (' This is ONLY ok if you '+
                'intend to exclude the LAST {} blocks in the Thor output.'
                ).format(n_blocks_from_thorsync - n_blocks_from_gsheet)
            )
        else:
            raise ValueError(err_msg.format('<') + fail_msg)

    if allow_gsheet_to_restrict_blocks:
        # TODO unit test for case where first_block != 0 and == 0
        # w/ last_block == first_block and > first_block
        # TODO TODO doesn't this only support dropping blocks at end?
        # do i assert that first_block is 0 then? probably should...
        # TODO TODO TODO shouldnt it be first_block:last_block+1?
        block_first_frames = block_first_frames[
            :(last_block - first_block + 1)
        ]
        block_last_frames = block_last_frames[
            :(last_block - first_block + 1)
        ]
        assert len(block_first_frames) == n_blocks_from_gsheet
        assert len(block_last_frames) == n_blocks_from_gsheet

        odor_onset_frames = odor_onset_frames[
            :(last_presentation - first_presentation + 1)
        ]
        odor_offset_frames = odor_offset_frames[
            :(last_presentation - first_presentation + 1)
        ]
        del first_presentation, last_presentation
        frame_times = frame_times[:(block_last_frames[-1] + 1)]

    # TODO TODO TODO need to adjust odor_onset_frames to exclude last
    # presentations missing at end of each block, if not same len as odor
    # list
    n_missing_presentations = len(odor_list) - len(odor_onset_frames)
    assert n_missing_presentations >= 0

    if allow_missing_odor_presentations and n_missing_presentations > 0:
        # MATLAB code also assumes equal number missing in each block.
        assert n_missing_presentations % n_blocks_from_gsheet == 0
        n_missing_per_block = \
            n_missing_presentations // n_blocks_from_gsheet

        warnings.warn('{} missing presentations per block!'.format(
            n_missing_per_block
        ))
        n_deleted = 0
        for i in range(n_blocks_from_gsheet):
            end_plus_one = presentations_per_block * (i + 1) - n_deleted
            del_start = end_plus_one - n_missing_per_block
            del_stop = end_plus_one - 1

            to_delete = odor_list[del_start:(del_stop + 1)]
            warnings.warn('presentations {} to {} ({}) were missing'.format(
                del_start + 1 + n_deleted, del_stop + 1 + n_deleted,
                to_delete
            ))
            n_deleted += len(to_delete)
            del odor_list[del_start:(del_stop + 1)]
            del odor_ids[del_start:(del_stop + 1)]

        presentations_per_block -= n_missing_per_block

    # TODO move these three checks to a fn that checks timing info against
    # stimfile
    n_presentations = n_blocks_from_gsheet * presentations_per_block
    assert (len(odor_onset_frames) ==
        (n_presentations - n_missing_presentations)
    )
    assert (len(odor_offset_frames) ==
        (n_presentations - n_missing_presentations)
    )
    del n_presentations, n_missing_presentations

    assert len(odor_onset_frames) == len(odor_list)

    assert len(odor_ids) == len(odor_list)

    if verbose:
        print_trial_odors(data, odor_onset_frames)
        print('')

    end = time.time()
    print('Loading metadata took {:.3f} seconds'.format(end - start))

    # TODO TODO any way to only del existing movie if required to have
    # enough memory to load the new one (referring to how this code worked when
    # it was still a part of gui.py)?
    print('Loading TIFF {}...'.format(tiff), end='', flush=True)
    start = time.time()
    # TODO maybe just load a range of movie (if not all blocks/frames used)?
    # TODO is cnmf expecting float to be in range [0,1], like skimage?
    movie = tifffile.imread(tiff).astype('float32')
    end = time.time()
    print(' done')
    print('Loading TIFF took {:.3f} seconds'.format(end - start))

    # TODO TODO TODO fix what is causing more elements in frame_times than i
    # expect (in matlab/matlab_kc_plane/get_stiminfo.m) and delete this hack 
    n_flyback_frames = thor.get_thorimage_n_flyback_xml(xml)
    del xml
    if n_flyback_frames > 0:
        assert len(movie.shape) == 4
        z_total = movie.shape[1] + n_flyback_frames

        # this should be effectively taking the min within each stride
        frame_times = frame_times[::z_total]
        
        # these are the bigger opportunity for error
        frame_times = frame_times[:movie.shape[0]]
        # assuming frame_times was not modified earlier. if keeping this hack,
        # would want to at least move it before earlier possible modifications,
        # and then delete this line.
        orig_frame_times = frame_times.copy()

        step = int(len(frame_times) / 3)
        block_first_frames = np.arange(len(frame_times) - step + 1, step=step)
        block_last_frames = np.arange(step - 1, len(frame_times), step=step)

        orig_block_first_frames = block_first_frames.copy()
        orig_block_last_frames = block_last_frames.copy()

        odor_onset_frames = np.round(odor_onset_frames / z_total
            ).astype(np.uint16)
        odor_offset_frames = np.round(odor_offset_frames / z_total
            ).astype(np.uint16)

    # TODO may need to remove this assert to handle cases where there is a
    # partial block (stopped early). still check after slicing tho.
    # (warn instead, probably) (add a flag to just warn?)
    matlab.check_movie_timing_info(movie, orig_frame_times,
        orig_block_first_frames, orig_block_last_frames
    )
    del orig_frame_times, orig_block_first_frames, orig_block_last_frames

    # TODO probably delete after i come up w/ a better way to handle splitting
    # movies and analyzing subsets of them.  this is just to get the frame #s to
    # subset tiff in imagej
    # Printing before `drop_first_n_frames` is subtracted, otherwise frame
    # numbers would not be correct.
    # TODO shouldn't i move this before movie loading if possible, as some of
    # the other prints? (flag to loading fn to do this?)
    if verbose:
        print_block_frames(block_first_frames, block_last_frames)

    last_frame = block_last_frames[-1]
    # TODO TODO should they really not be considered part of the last block
    # in this case...?
    n_tossed_frames = movie.shape[0] - (last_frame + 1)
    if n_tossed_frames != 0:
        warnings.warn(('Tossing trailing {} of {} frames of movie, which'
            ' did not belong to any used block.\n').format(
            n_tossed_frames, movie.shape[0]
        ))
    del n_tossed_frames

    odor_onset_frames = [n - drop_first_n_frames
        for n in odor_onset_frames
    ]
    odor_offset_frames = [n - drop_first_n_frames
        for n in odor_offset_frames
    ]
    block_first_frames = [n - drop_first_n_frames
        for n in block_first_frames
    ]
    # TODO TODO TODO why was i doing this? after subtracting one, is this
    # still not true??? (fix!)
    # i feel like this might mean the rest of my handling of this case might
    # be incorrect...
    #block_first_frames[0] = 0
    # TODO delete after addressing the above. maybe move a check like this
    # to `load_mat_timing_info`
    assert block_first_frames[0] == 0
    #

    block_last_frames = [n - drop_first_n_frames
        for n in block_last_frames
    ]

    frame_times = frame_times[drop_first_n_frames:]
    # TODO TODO TODO is it correct that we were using the last_frame defined
    # before drop_first_n_frames wasa subtracted from everything???
    # TODO want / need to do more than just slice to free up memory from
    # other pixels? is that operation worth it?
    movie = movie[drop_first_n_frames:(last_frame + 1)]

    # This check is now an assert in matlab.check_movie_timing_info call below.
    # May need to allow that to be switched to a warning, if this failure
    # mode still exists.
    '''
    if movie.shape[0] != len(frame_times):
        warnings.warn('{} != {}'.format(movie.shape[0], len(frame_times)))
    '''
    matlab.check_movie_timing_info(movie, frame_times, block_first_frames,
        block_last_frames
    )

    trial_start_frames, trial_stop_frames = util.assign_frames_to_trials(
        movie, presentations_per_block, block_first_frames, odor_onset_frames
    )
    # TODO probably do want a fn that can return movie and metadata, so that
    # other segmentation functions can be connected to that...
    # (not clear on what best representation would be, however)
    ############################################################################
    # End what originally happened in gui.py/Segmentation.open_recording
    ############################################################################

    ############################################################################
    # Copied from gui load_ijois
    ############################################################################

    # TODO delete this hardcode hack
    if len(movie.shape) == 4:
        assert tiff.startswith(raw_data_root())
        ijroiset_filename = join(image_dir, 'rois.zip')
        assert exists(ijroiset_filename)
    #
    else:
        # TODO maybe factor this getting roi filname, reading rois, making masks
        # [, extracting traces] into a fn?
        ijroiset_filename = tiff_ijroi_filename(tiff)
        if ijroiset_filename is None:
            raise IOError('tiff_ijroi_filename returned None')

    # TODO may need to use this mtime later (particularly if we enter into
    # db as before in gui)
    # (was set into self.run_at)
    # (also set parameter_json and run_len_seconds to None)
    ijroiset_mtime = datetime.fromtimestamp(getmtime(ijroiset_filename))

    ijrois = ijroi.read_roi_zip(ijroiset_filename)

    frame_shape = movie.shape[1:]
    footprints = ijrois2masks(ijrois, frame_shape)

    raw_f = extract_traces_boolean_footprints(movie, footprints)
    #n_footprints = raw_f.shape[1]

    df_over_f = calculate_df_over_f(raw_f, trial_start_frames,
        odor_onset_frames, trial_stop_frames
    )

    ############################################################################
    # What was originally in gui.py/Segmentation.get_recording_dfs
    ############################################################################
    n_frames, n_cells = df_over_f.shape
    # would have to pass footprints back / read from sql / read # from sql
    #assert n_cells == n_footprints
    # TODO bring back after fixing this indexing issue,
    # whatever it is. as with other check in open_recording
    # (mostly redundant w/ assert comparing movie frames and frame_times in
    # end of open_recording...)
    #assert frame_times.shape[0] == n_frames

    presentation_dfs = []
    comparison_dfs = []
    comparison_num = -1

    # TODO consider deleting this conditional if i'm not actually going to
    # support else case (not used now, see where repeat_num is set in loop)
    if pair_case:
        repeats_across_real_blocks = False
    else:
        repeats_across_real_blocks = True
        repeat_nums = {id_group: 0 for id_group in odor_ids}

    print('processing presentations...', end='', flush=True)
    for i in range(len(trial_start_frames)):
        if i % presentations_per_block == 0:
            comparison_num += 1
            if not repeats_across_real_blocks:
                repeat_nums = {id_group: 0 for id_group in odor_ids}

        start_frame = trial_start_frames[i]
        stop_frame = trial_stop_frames[i]
        onset_frame = odor_onset_frames[i]
        offset_frame = odor_offset_frames[i]

        assert start_frame < onset_frame
        assert onset_frame < offset_frame
        assert offset_frame < stop_frame

        # If either of these is out of bounds, presentation_frametimes will
        # just be shorter than it should be, but it would not immediately
        # make itself apparent as an error.
        assert start_frame < len(frame_times)
        assert stop_frame < len(frame_times)

        onset_time = frame_times[onset_frame]
        # TODO TODO check these don't jump around b/c discontinuities
        # TODO TODO TODO honestly, i forget now, have i ever had acquisition
        # stop any time other than between "blocks"? do i want to stick to
        # that definition?
        # if it did only ever stop between blocks, i suppose i'm gonna have
        # to paint frames between trials within a block as belonging to one
        # trial or the other, for purposes here...
        presentation_frametimes = \
            frame_times[start_frame:stop_frame] - onset_time

        curr_odor_ids = odor_ids[i]
        # TODO update if odor ids are ever actually allowed to be arbitrary
        # len list (and not just forced to be length-2 as they are now, b/c
        # of the db mixture table design)
        odor1, odor2 = curr_odor_ids
        #

        if pair_case:
            repeat_num = repeat_nums[curr_odor_ids]
            repeat_nums[curr_odor_ids] = repeat_num + 1

        # See note in missing odor handling portion of
        # process_segmentation_output to see reasoning behind this choice.
        else:
            repeat_num = comparison_num

        # TODO check that all frames go somewhere and that frames aren't
        # given to two presentations. check they stay w/in block boundaries.
        # (they don't right now. fix!)

        date, fly_num = keys[:2]

        # TODO share more of this w/ dataframe creation below, unless that
        # table is changed to just reference presentation table
        presentation = pd.DataFrame({
            # TODO fix hack (what was this for again? it really a problem?)
            'temp_presentation_id': [i],
            'prep_date': [date],
            'fly_num': fly_num,
            'recording_from': started_at,
            'analysis': ijroiset_mtime, #run_at,
            # TODO get rid of this hack after fixing earlier association of
            # blocks / repeats (or fixing block structure for future
            # recordings)
            'comparison': comparison_num if pair_case else 0,
            'real_block': comparison_num,
            'odor1': odor1,
            'odor2': odor2,
            #'repeat_num': repeat_num if pair_case else comparison_num,
            'repeat_num': repeat_num,
            'odor_onset_frame': onset_frame,
            'odor_offset_frame': offset_frame,
            'from_onset': [[float(x) for x in presentation_frametimes]],
            # TODO now that this isn't in the gui, probably make this start
            # NULL / False?
            'presentation_accepted': True
        })

        # TODO TODO assert that len(presentation_frametimes)
        # == stop_frame - start_frame (off-by-one?)
        # TODO (it would fail now) fix!!
        # maybe this is a failure to merge correctly later???
        # b/c presentation frametimes seems to be defined to be same length
        # above... same indices...
        # (unless maybe frame_times is sometimes shorter than df_over_f, etc)

        '''
        presentation_dff = df_over_f[start_frame:stop_frame, :]
        presentation_raw_f = raw_f[start_frame:stop_frame, :]
        '''
        # TODO TODO fix / delete hack!!
        # TODO probably just need to more correctly calculate stop_frame?
        # (or could also try expanding frametimes to include that...)
        actual_frametimes_slice_len = len(presentation_frametimes)
        stop_frame = start_frame + actual_frametimes_slice_len
        presentation_dff = df_over_f[start_frame:stop_frame, :]
        presentation_raw_f = raw_f[start_frame:stop_frame, :]

        # Assumes that cells are indexed same here as in footprints.
        cell_dfs = []
        for cell_num in range(n_cells):

            cell_dff = presentation_dff[:, cell_num].astype('float32')
            cell_raw_f = presentation_raw_f[:, cell_num].astype('float32')

            cell_dfs.append(pd.DataFrame({
                # TODO maybe rename / do in a less hacky way
                'temp_presentation_id': [i],
                ###'presentation_id': [presentation_id],
                'recording_from': [started_at],
                'segmentation_run': [ijroiset_mtime], #run_at],
                'cell': [cell_num],
                'df_over_f': [[float(x) for x in cell_dff]],
                'raw_f': [[float(x) for x in cell_raw_f]]
            }))
        response_df = pd.concat(cell_dfs, ignore_index=True)

        # TODO maybe draw correlations from each of these, as i go?
        # (would still need to do block by block, not per trial)

        presentation_dfs.append(presentation)
        # TODO rename...
        comparison_dfs.append(response_df)
    print(' done', flush=True)

    # TODO would need to fix coo_matrix handling in 3d+t case
    # (may not be possible w/ coo_matrix...)
    ''''
    n_footprints = footprints.shape[-1]
    footprint_dfs = []
    for cell_num in range(n_footprints):
        # TODO could use tuple of slice objects to accomodate arbitrary dims
        # here (x,y,Z). change all places like this.
        sparse = coo_matrix(footprints[:,:,cell_num])
        footprint_dfs.append(pd.DataFrame({
            'recording_from': [started_at],
            'segmentation_run':  [ijroiset_mtime], #run_at],
            'cell': [cell_num],
            # Can be converted from lists of Python types, but apparently
            # not from numpy arrays or lists of numpy scalar types.
            # TODO check this doesn't transpose things
            # TODO just move appropriate casting to my to_sql function,
            # and allow having numpy arrays (get type info from combination
            # of that and the database, like in other cases)
            # TODO TODO TODO TODO was sparse.col for x_* and sparse.row for
            # y_*. I think this was why I needed to tranpose footprints
            # sometimes. fix everywhere.
            'x_coords': [[int(x) for x in sparse.row.astype('int16')]],
            'y_coords': [[int(x) for x in sparse.col.astype('int16')]],
            'weights': [[float(x) for x in sparse.data.astype('float32')]]
        }))
    footprint_df = pd.concat(footprint_dfs, ignore_index=True)
    '''

    ############################################################################
    # From gui.py/process_segmentation_output
    ############################################################################
    presentations_df = pd.concat(presentation_dfs, ignore_index=True)

    # TODO TODO TODO do i really need to recalculate these?

    # TODO TODO TODO probably just fix self.n_blocks earlier
    # in supermixture case
    # (so there is only one button for accepting and stuff...)
    if pair_case:
        n_blocks = n_blocks
        presentations_per_block = presentations_per_block
    else:
        # TODO delete (though check commented and new are equiv on all
        # non pair_case experiments)
        '''
        n_blocks = presentations_df.comparison.max() + 1
        n_repeats = n_expected_repeats(presentations_df)
        n_stim = len(presentations_df[['odor1','odor2']].drop_duplicates())
        presentations_per_block = n_stim * n_repeats
        '''
        #
        # TODO TODO TODO is this really what i want?
        n_blocks = 1
        presentations_per_block = len(odor_ids)

    presentations_df = merge_odors(presentations_df, db_odors.reset_index())

    # TODO maybe adapt to case where name2 might have only occurence of 
    # an odor, or name1 might be paraffin.
    # TODO TODO check this is actually in the order i want across blocks
    # (idk if name1,name2 are sorted / re-ordered somewhere)
    name1_unique = presentations_df.name1.unique()
    name2_unique = presentations_df.name2.unique()
    # TODO should fail earlier (rather than having to wait for cnmf
    # to finish)
    assert (set(name2_unique) == {'no_second_odor'} or 
        set(name2_unique) - set(name1_unique) == {'paraffin'}
    )
    # TODO TODO TODO factor all abbreviation into its own function
    # (which transforms dataframe w/ full odor names / ids maybe to
    # df w/ the additional abbreviated col (or renamed col))
    single_letter_abbrevs = False
    abbrev_in_presentation_order = True
    if single_letter_abbrevs:
        if not abbrev_in_presentation_order:
            # TODO would (again) need to look up fixed desired order, as
            # in kc_mix_analysis, to support this
            raise NotImplementedError
    else:
        if abbrev_in_presentation_order:
            warnings.warn('abbrev_in_presentation_order can only '
                'be False if not using single_letter_abbrevs'
            )

    if not abbrev_in_presentation_order:
        # TODO would (again) need to look up fixed desired order, as
        # in kc_mix_analysis, to support this
        raise NotImplementedError
        # (could implement other order by just reorder name1_unique)

    # TODO probably move this fn from chemutils to hong2p.utils
    odor2abbrev = cu.odor2abbrev_dict(name1_unique,
        single_letter_abbrevs=single_letter_abbrevs
    )

    # TODO rewrite later stuff to avoid need for this.
    # it just adds a bit of confusion at this point.
    # TODO need to deal w/ no_second_odor in here?
    # So that code detecting which combinations of name1+name2 are
    # monomolecular does not need to change.
    # TODO TODO doesn't chemutils do this at this point? test
    odor2abbrev['paraffin'] = 'paraffin'
    # just so name2 isn't all NaN for now...
    odor2abbrev['no_second_odor'] = 'no_second_odor'

    block_iter = list(range(n_blocks))

    for i in block_iter:
        # TODO maybe concat and only set whole df as instance variable in
        # get_recording_df? then use just as in kc_analysis all throughout
        # here? (i.e. subset from presentationS_df above...)
        presentation_dfs = presentation_dfs[
            (presentations_per_block * i):
            (presentations_per_block * (i + 1))
        ]
        presentation_df = pd.concat(presentation_dfs,
            ignore_index=True
        )
        comparison_dfs = comparison_dfs[
            (presentations_per_block * i):
            (presentations_per_block * (i + 1))
        ]

        # Using this in place of NaN, so frame nums will still always have
        # int dtype. maybe NaN would be better though...
        # TODO maybe i want something unlikely to be system dependent
        # though... if i'm ever going to serialize something containing
        # this value...
        INT_NO_REAL_FRAME = sys.maxsize
        last_real_temp_id = presentation_df.temp_presentation_id.max()

        # Not supporting filling in missing odor presentations in pair case
        # (it hasn't happened yet). (and would need to consider within
        # comparisons, since odors be be shared across)
        if not pair_case:
            # TODO maybe add an 'actual_block' column or something in
            # paircase? or in both?
            assert presentation_df.comparison.nunique() == 1
            n_full_repeats = presentation_df.odor1.value_counts().max()
            assert len(presentation_df) % n_full_repeats == 0

            odor1_set = set(presentation_df.odor1)
            n_odors = len(odor1_set)

            # n_blocks and n_pres_per_actual_block currently have different
            # meanings in pair_case and not here, w/ blocks in this case not
            # matching actual "scopePin high" blocks.
            # In this case, "actual blocks" have each odor once.
            n_pres_per_actual_block = len(presentation_df) // n_full_repeats

            n_missed_per_block = n_odors - n_pres_per_actual_block

            # TODO TODO add a flag for whether we should fill in missing
            # data like this, and maybe fail if the flag is false and we
            # have missing data (b/c plot labels get screwed up)
            # (don't i already have a flag in open_recording? make more
            # global (or an instance variable)?)

            # TODO may want to assert all odor id lookups work in merge
            # (if it doesn't already functionally do that), because
            # technically it's possible that it just so happens the last
            # odor (the missed one) is always the same

            if n_missed_per_block > 0:
                # Could modify loop below to iterate over missed odors if
                # want to support this.
                assert n_missed_per_block == 1, 'for simplicity'

                # from_onset is not hashable so nunique on everything fails
                const_cols = presentation_df.columns[[
                    (False if c == 'from_onset' else
                    presentation_df[c].nunique() == 1)
                    for c in presentation_df.columns
                ]]
                const_vals = presentation_df[const_cols].iloc[0].to_dict()

                # TODO if i were to support where n_cells being different in
                # each block, would need to subset comparison df to block
                # and get unique values from there (in loop below)
                cells = comparison_dfs[0].cell.unique()
                rec_from = const_vals['recording_from']
                filler_seg_run = pd.NaT

                pdf_in_order = \
                    presentation_df.sort_values('odor_onset_frame')

                next_filler_temp_id = last_real_temp_id + 1
                for b in range(n_full_repeats):
                    start = b * n_pres_per_actual_block
                    stop = (b + 1) * n_pres_per_actual_block
                    bdf = pdf_in_order[start:stop]

                    row_data = dict(const_vals)
                    row_data['from_onset'] = [np.nan]
                    # Careful! This should be cleared after frame2order def.
                    row_data['odor_onset_frame'] = \
                        bdf.odor_onset_frame.max() + 1
                    row_data['odor_offset_frame'] = INT_NO_REAL_FRAME

                    real_block_nums = bdf.real_block.unique()
                    assert len(real_block_nums) == 1
                    real_block_num = real_block_nums[0]
                    row_data['real_block'] = real_block_num

                    # The question here is whether I want to start the
                    # repeat numbering with presentations that actually have
                    # frames, or whether I want to keep the numbering as it
                    # would have been...

                    # Since in !pair_case, real_block num should be
                    # equal to the intended repeat_num.
                    row_data['repeat_num'] = real_block_num
                    # TODO would need to fix this case to handle multiple
                    # missing of one odor, if i did want to have repeat_num
                    # numbering start with presentations that actually have
                    # frames
                    # (- 1 since 0 indexed)
                    #row_data['repeat_num'] = n_full_repeats - 1

                    missing_odor1s = list(odor1_set - set(bdf.odor1))
                    assert len(missing_odor1s) == 1
                    missing_odor1 = missing_odor1s.pop()
                    row_data['odor1'] = missing_odor1

                    row_data['temp_presentation_id'] = next_filler_temp_id

                    presentation_df = \
                        presentation_df.append(row_data, ignore_index=True)

                    # TODO what's the np.nan stuff here for?
                    # why not left to get_recording_dfs?
                    comparison_dfs.append(pd.DataFrame({
                        'temp_presentation_id': next_filler_temp_id,
                        'recording_from': rec_from,
                        'segmentation_run': filler_seg_run,
                        'cell': cells,
                        'raw_f': [[np.nan] for _ in range(len(cells))],
                        'df_over_f': [[np.nan] for _ in range(len(cells))]
                    }))
                    next_filler_temp_id += 1

    frame2order = {f: o for o, f in
        enumerate(sorted(presentation_df.odor_onset_frame.unique()))
    }
    presentation_df['order'] = \
        presentation_df.odor_onset_frame.map(frame2order)
    del frame2order

    # This does nothing if there were no missing odor presentations.
    presentation_df.loc[
        presentation_df.temp_presentation_id > last_real_temp_id,
        'odor_onset_frame'] = INT_NO_REAL_FRAME

    comparison_df = pd.concat(comparison_dfs, ignore_index=True,
        sort=False
    )

    # TODO don't have separate instance variables for presentation_dfs
    # and comparison_dfs if i'm always going to merge here.
    # just merge before and then put in one instance variable.
    # (probably just keep name comparison_dfs)
    presentation_df['from_onset'] = presentation_df['from_onset'].apply(
        lambda x: np.array(x)
    )
    presentation_df = merge_odors(presentation_df, db_odors.reset_index())

    # TODO maybe only abbreviate at end? this approach break upload to
    # database? maybe redo so abbrev only happens before plot?
    # (may want a consistent order across experiments anyway)
    presentation_df['original_name1'] = presentation_df.name1.copy()
    presentation_df['original_name2'] = presentation_df.name2.copy()

    presentation_df['name1'] = presentation_df.name1.map(odor2abbrev)
    presentation_df['name2'] = presentation_df.name2.map(odor2abbrev)

    presentation_df = merge_recordings(presentation_df, recording_df)

    # TODO TODO TODO assert here, and earlier if necessary, that
    # each odor has all repeat_num + ordering of repeat_num matches
    # that of 'order' column
    #comparison_df[['name1','repeat_num','order']
    #].drop_duplicates().sort_values(['name1','repeat_num','order'])

    # Just including recording_from so it doesn't get duplicated in
    # output (w/ '_x' and '_y' suffixes). This checks recording_from
    # values are all equal, rather than just dropping one.
    # No other columns should be common.
    comparison_df = comparison_df.merge(presentation_df,
        left_on=['recording_from', 'temp_presentation_id'],
        right_on=['recording_from', 'temp_presentation_id']
    )
    comparison_df.drop(columns='temp_presentation_id', inplace=True)
    del presentation_df

    comparison_df = util.expand_array_cols(comparison_df)

    # TODO TODO make this optional
    # (and probably move to upload where fig gets saved.
    # just need to hold onto a ref to comparison_df)
    #df_filename = (run_at.strftime('%Y%m%d_%H%M_') +
    df_filename = (ijroiset_mtime.strftime('%Y%m%d_%H%M_') +
        recording_title.replace('/','_') + '.p'
    )
    df_filename = join(analysis_output_root(), 'trace_pickles',
        df_filename
    )

    print('writing dataframe to {}...'.format(df_filename), end='',
        flush=True
    )
    # TODO TODO write a dict pointing to this, to also include PID
    # information in another variable?? or at least stuff to index
    # the PID information?
    comparison_df.to_pickle(df_filename)
    print(' done', flush=True)

    # TODO TODO TODO only return dataframes?
    return comparison_df


